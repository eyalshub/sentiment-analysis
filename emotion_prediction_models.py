# -*- coding: utf-8 -*-
"""Emotion prediction models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14zQk6efbd1Q_OkJnBk3vzrlMg2zb_vet
"""

import nltk
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
# Download stop words list and punkt tokenizer (if not previously downloaded)
nltk.download('stopwords')
nltk.download('punkt')
!pip install gensim
!pip install pyLDAvis
!pip install wordcloud matplotlib

!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv
!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv
!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv

goemotions_1 = pd.read_csv('data/full_dataset/goemotions_1.csv')
goemotions_2 = pd.read_csv('data/full_dataset/goemotions_2.csv')
goemotions_3 = pd.read_csv('data/full_dataset/goemotions_3.csv')

combined_df = pd.concat([goemotions_1, goemotions_2, goemotions_3], ignore_index=True)
emotion_columns = combined_df.columns[9:]

combined_df['created_utc'] = pd.to_datetime(combined_df['created_utc'], unit='s', errors='coerce')
combined_df.to_csv('data/full_dataset/goemotions_combined.csv', index=False)



from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict
import re
import nltk
from nltk.corpus import stopwords
from collections import defaultdict, Counter
import pandas as pd

def preprocess_text(text, stop_words):
    text = re.sub(r'\W+', ' ', text)
    words = text.lower().split()
    words = [word for word in words if word not in stop_words]
    return words


def get_most_common_words(text_series, top_n=20):
    all_words = []
    for text in text_series:
        words = text.split()
        all_words.extend(words)

    word_counts = Counter(all_words)
    most_common_words = word_counts.most_common(top_n)
    return [word for word, _ in most_common_words]


def build_emotion_word_dict(df, text_column, emotion_columns, top_n_words=20):
    stop_words = list(stopwords.words('english'))

    emotion_word_dict = defaultdict(dict)

    for emotion in emotion_columns:

        emotion_df = df[df[emotion] == 1][text_column]


        processed_texts = emotion_df.apply(lambda x: preprocess_text(x, stop_words)).tolist()


        processed_texts = [' '.join(text) for text in processed_texts if text]

        if len(processed_texts) == 0:
            print(f"No valid texts found for emotion: {emotion}")
            continue


        tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=top_n_words)
        try:
            tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)
            top_tfidf_words = tfidf_vectorizer.get_feature_names_out()
            emotion_word_dict[emotion]['tfidf_words'] = list(top_tfidf_words)
        except ValueError as e:
            print(f"Error processing emotion '{emotion}': {e}")
            emotion_word_dict[emotion]['tfidf_words'] = []


        common_words = get_most_common_words(processed_texts, top_n=top_n_words)
        emotion_word_dict[emotion]['common_words'] = common_words

    return emotion_word_dict


text_column = 'text'

emotion_word_dict = build_emotion_word_dict(combined_df, text_column, emotion_columns, top_n_words=20)

for emotion, words in emotion_word_dict.items():
    tfidf_words = ', '.join(words.get('tfidf_words', []))
    common_words = ', '.join(words.get('common_words', []))
    print(f"Top words for {emotion}:")
    print(f"  TF-IDF Words: {tfidf_words}")
    print(f"  Common Words: {common_words}")
    print("\n")

def emotion_word_dict_to_df(emotion_word_dict):
    data = []

    for emotion, words in emotion_word_dict.items():
        tfidf_words = ', '.join(words.get('tfidf_words', []))
        common_words = ', '.join(words.get('common_words', []))

        data.append([emotion, tfidf_words, common_words])

    df = pd.DataFrame(data, columns=['Emotion', 'TF-IDF Words', 'Common Words'])

    return df

emotion_word_df = emotion_word_dict_to_df(emotion_word_dict)

import json
import os
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter, defaultdict
import re
import nltk
from nltk.corpus import stopwords
import pandas as pd

def preprocess_text(text, stop_words):
    text = re.sub(r'\W+', ' ', text)
    words = text.lower().split()
    words = [word for word in words if word not in stop_words]
    return words

stop_words = list(stopwords.words('english'))

def get_most_common_words(text_series, top_n=20):
    all_words = []
    for text in text_series:
        words = text.split()
        all_words.extend(words)
    word_counts = Counter(all_words)
    most_common_words = word_counts.most_common(top_n)
    return [word for word, _ in most_common_words]

def compute_metrics_for_emotion(emotion, df, text_column, top_n_words=20):
    stop_words = list(stopwords.words('english'))

    emotion_df = df[df[emotion] == 1][text_column]

    if emotion_df.empty:
        print(f"No data for emotion: {emotion}")
        return {emotion: {'tfidf_words': [], 'common_words': [], 'log_tf': {}}}

    processed_texts = emotion_df.apply(lambda x: preprocess_text(x, stop_words)).tolist()
    processed_texts = [' '.join(text) for text in processed_texts if text]

    if len(processed_texts) == 0:
        print(f"No valid texts for emotion: {emotion}")
        return {emotion: {'tfidf_words': [], 'common_words': [], 'log_tf': {}}}

    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=top_n_words)
    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)
    top_tfidf_words = tfidf_vectorizer.get_feature_names_out()

    common_words = get_most_common_words(processed_texts, top_n=top_n_words)

    word_indices = {word: idx for idx, word in enumerate(top_tfidf_words)}
    log_tf = {word: np.log1p(tfidf_matrix[:, word_indices[word]].sum()) for word in top_tfidf_words}

    return {
        emotion: {
            'tfidf_words': list(top_tfidf_words),
            'common_words': common_words,
            'log_tf': log_tf
        }
    }

def save_results_to_file(results, file_path):
    with open(file_path, 'w') as f:
        json.dump(results, f)

def load_results_from_file(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return {}

def compute_metrics_for_all_data(df, text_column, top_n_words=100):
    stop_words = list(stopwords.words('english'))

    all_texts = df[text_column].dropna().apply(lambda x: preprocess_text(x, stop_words)).tolist()
    processed_texts = [' '.join(text) for text in all_texts if text]

    if len(processed_texts) == 0:
        print("No valid texts in the dataset.")
        return {'tfidf_words': [], 'common_words': [], 'log_tf': {}}

    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=top_n_words)
    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)
    top_tfidf_words = tfidf_vectorizer.get_feature_names_out()

    common_words = get_most_common_words(processed_texts, top_n=top_n_words)

    word_indices = {word: idx for idx, word in enumerate(top_tfidf_words)}
    log_tf = {word: np.log1p(tfidf_matrix[:, word_indices[word]].sum()) for word in top_tfidf_words}

    return {
        'tfidf_words': list(top_tfidf_words),
        'common_words': common_words,
        'log_tf': log_tf
    }

results = load_results_from_file('emotion_word_metrics.json')
for emotion in emotion_columns:
    if emotion in results:
        print(f"Skipping {emotion}, already processed.")
        continue

    print(f"Processing emotion: {emotion}")
    emotion_result = compute_metrics_for_emotion(emotion, combined_df, text_column='text', top_n_words=100)
    results.update(emotion_result)

    save_results_to_file(results, 'emotion_word_metrics.json')

print("Processing all data...")
all_data_metrics = compute_metrics_for_all_data(combined_df, text_column='text', top_n_words=100)
results['all_data'] = all_data_metrics

save_results_to_file(results, 'emotion_word_metrics.json')

print("Processing completed.")

import json


file_path = 'emotion_word_metrics.json'

with open(file_path, 'r') as f:
    emotion_metrics = json.load(f)

for emotion, metrics in emotion_metrics.items():
    print(f"Emotion: {emotion}")
    print(f"  TF-IDF Words: {', '.join(metrics['tfidf_words'])}")
    print(f"  Common Words: {', '.join(metrics['common_words'])}")
    print(f"  LogTF: {metrics['log_tf']}")

import json
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

def load_results_from_file(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return {}

def plot_tfidf(results, emotion, top_n=10):
    tfidf_words = results[emotion]['tfidf_words']
    log_tf_values = results[emotion]['log_tf']

    sorted_words = sorted(log_tf_values.items(), key=lambda x: x[1], reverse=True)[:top_n]
    words, scores = zip(*sorted_words)

    plt.figure(figsize=(10, 6))
    sns.barplot(x=list(scores), y=list(words), palette="Blues_d")
    plt.title(f"Top {top_n} TF-IDF Words for Emotion: {emotion}")
    plt.xlabel('LogTF Value')
    plt.ylabel('Words')
    plt.show()

def plot_common_words(results, emotion, top_n=10):
    common_words = results[emotion]['common_words']

    plt.figure(figsize=(10, 6))
    sns.countplot(y=common_words[:top_n], palette="viridis")
    plt.title(f"Top {top_n} Common Words for Emotion: {emotion}")
    plt.xlabel('Frequency')
    plt.ylabel('Words')
    plt.show()

def plot_wordcloud(results, emotion):
    tfidf_words = results[emotion]['tfidf_words']
    word_freq = {word: results[emotion]['log_tf'].get(word, 0) for word in tfidf_words}

    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"WordCloud for Emotion: {emotion}")
    plt.show()

def plot_emotion_visualizations(results, top_n=10):
    for emotion in results:
        if emotion != "all_data":
            print(f"Generating plots for {emotion}...")
            plot_tfidf(results, emotion, top_n)
            plot_common_words(results, emotion, top_n)
            plot_wordcloud(results, emotion)

results = load_results_from_file('emotion_word_metrics.json')

plot_emotion_visualizations(results, top_n=10)

def plot_all_data_tfidf(results, top_n=10):
    if 'all_data' not in results:
        print("No 'all_data' found in the results.")
        return

    all_data_log_tf = results['all_data']['log_tf']

    sorted_words = sorted(all_data_log_tf.items(), key=lambda x: x[1], reverse=True)[:top_n]
    words, scores = zip(*sorted_words)

    plt.figure(figsize=(10, 6))
    sns.barplot(x=list(scores), y=list(words), palette="Blues_d")
    plt.title(f"Top {top_n} TF-IDF Words for All Data")
    plt.xlabel('LogTF Value')
    plt.ylabel('Words')
    plt.show()

def plot_all_data_common_words(results, top_n=10):
    if 'all_data' not in results:
        print("No 'all_data' found in the results.")
        return

    all_data_common_words = results['all_data']['common_words']

    plt.figure(figsize=(10, 6))
    sns.countplot(y=all_data_common_words[:top_n], palette="viridis")
    plt.title(f"Top {top_n} Common Words for All Data")
    plt.xlabel('Frequency')
    plt.ylabel('Words')
    plt.show()

def plot_all_data_wordcloud(results):
    if 'all_data' not in results:
        print("No 'all_data' found in the results.")
        return

    all_data_tfidf_words = results['all_data']['tfidf_words']
    word_freq = {word: results['all_data']['log_tf'].get(word, 0) for word in all_data_tfidf_words}

    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title("WordCloud for All Data")
    plt.show()

def plot_all_data_visualizations(results, top_n=10):
    print("Generating plots for all data...")
    plot_all_data_tfidf(results, top_n)
    plot_all_data_common_words(results, top_n)
    plot_all_data_wordcloud(results)

results = load_results_from_file('emotion_word_metrics.json')

plot_all_data_visualizations(results, top_n=10)



neutral_df  = combined_df[combined_df['neutral'] == 1]
whitout_neutral_df  = combined_df[combined_df['neutral'] == 0]
common_ids = set(neutral_df['id']).intersection(set(whitout_neutral_df['id']))
neutral_with_common_ids = neutral_df[neutral_df['id'].isin(common_ids)]
whitout_neutral_with_common_ids = whitout_neutral_df[whitout_neutral_df['id'].isin(common_ids)]
print(neutral_df.shape,whitout_neutral_df.shape,len(common_ids))
print(f"Size of neutral_with_common_ids: {neutral_with_common_ids.shape}")
print(f"Size of whitout_neutral_with_common_ids: {whitout_neutral_with_common_ids.shape}")

import pandas as pd
import re
import numpy as np
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer


def preprocess_text(text, stop_words):
    text = re.sub(r'\W+', ' ', text) 
    words = text.lower().split()   
    words = [word for word in words if word not in stop_words]  
    return words

stop_words = list(stopwords.words('english'))  

def compute_metrics_for_emotion(emotion, df, text_column, top_n_words=20):
    emotion_df = df[df[emotion] == 1][text_column]

    if emotion_df.empty:
        print(f"No data for emotion: {emotion}")
        return {emotion: {'tfidf_words': [], 'common_words': [], 'log_tf': {}}}

    processed_texts = emotion_df.apply(lambda x: preprocess_text(x, stop_words)).tolist()
    processed_texts = [' '.join(text) for text in processed_texts if text]  # ביטול טקסטים ריקים

    if len(processed_texts) == 0:
        print(f"No valid texts for emotion: {emotion}")
        return {emotion: {'tfidf_words': [], 'common_words': [], 'log_tf': {}}}

    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=top_n_words)

    if len(processed_texts) == 0:
        print(f"Skipping TF-IDF computation for {emotion} due to no valid processed text.")
        return {emotion: {'tfidf_words': [], 'common_words': [], 'log_tf': {}}}

    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)
    top_tfidf_words = tfidf_vectorizer.get_feature_names_out()

    common_words = get_most_common_words(processed_texts, top_n=top_n_words)

    word_indices = {word: idx for idx, word in enumerate(top_tfidf_words)}
    log_tf = {word: np.log1p(tfidf_matrix[:, word_indices[word]].sum()) for word in top_tfidf_words}

    return {
        emotion: {
            'tfidf_words': list(top_tfidf_words),
            'common_words': common_words,
            'log_tf': log_tf
        }
    }

def save_results_to_file(results, file_path):
    with open(file_path, 'w') as f:
        json.dump(results, f)

def load_results_from_file(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return {}

results = load_results_from_file('whitout_neutral_emotion_word_metrics.json')
for emotion in emotion_columns:
    if emotion in results:
        print(f"Skipping {emotion}, already processed.")
        continue

    print(f"Processing emotion: {emotion}")
    emotion_result = compute_metrics_for_emotion(emotion, whitout_neutral_with_common_ids, text_column='text', top_n_words=100)
    results.update(emotion_result)

    save_results_to_file(results, 'whitout_neutral_emotion_word_metrics.json')

print("Processing all data...")
all_data_metrics = compute_metrics_for_all_data(whitout_neutral_with_common_ids, text_column='text', top_n_words=100)
results['all_data'] = all_data_metrics

save_results_to_file(results, 'whitout_neutral_emotion_word_metrics.json')

print("Processing completed.")

import pandas as pd
import re
import numpy as np
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

def preprocess_text(text, stop_words):
    text = re.sub(r'\W+', ' ', text) 
    words = text.lower().split()    
    words = [word for word in words if word not in stop_words]  
    return words

stop_words = list(stopwords.words('english'))  

def compute_metrics_for_emotion(emotion, df, text_column, top_n_words=20):
    emotion_df = df[df[emotion] == 1][text_column]

    if emotion_df.empty:
        print(f"No data for emotion: {emotion}")
        return {emotion: {'tfidf_words': [], 'common_words': [], 'log_tf': {}}}

    processed_texts = emotion_df.apply(lambda x: preprocess_text(x, stop_words)).tolist()
    processed_texts = [' '.join(text) for text in processed_texts if text] 

    if len(processed_texts) == 0:
        print(f"No valid texts for emotion: {emotion}")
        return {emotion: {'tfidf_words': [], 'common_words': [], 'log_tf': {}}}

    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=top_n_words)

    if len(processed_texts) == 0:
        print(f"Skipping TF-IDF computation for {emotion} due to no valid processed text.")
        return {emotion: {'tfidf_words': [], 'common_words': [], 'log_tf': {}}}

    tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)
    top_tfidf_words = tfidf_vectorizer.get_feature_names_out()

    common_words = get_most_common_words(processed_texts, top_n=top_n_words)

    word_indices = {word: idx for idx, word in enumerate(top_tfidf_words)}
    log_tf = {word: np.log1p(tfidf_matrix[:, word_indices[word]].sum()) for word in top_tfidf_words}

    return {
        emotion: {
            'tfidf_words': list(top_tfidf_words),
            'common_words': common_words,
            'log_tf': log_tf
        }
    }

def save_results_to_file(results, file_path):
    with open(file_path, 'w') as f:
        json.dump(results, f)

def load_results_from_file(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            return json.load(f)
    return {}

results = load_results_from_file('whit_neutral_emotion_word_metrics.json')
for emotion in emotion_columns:
    if emotion in results:
        print(f"Skipping {emotion}, already processed.")
        continue

    print(f"Processing emotion: {emotion}")
    emotion_result = compute_metrics_for_emotion(emotion, neutral_with_common_ids, text_column='text', top_n_words=100)
    results.update(emotion_result)

    save_results_to_file(results, 'whit_neutral_emotion_word_metrics.json')

print("Processing all data...")
all_data_metrics = compute_metrics_for_all_data(neutral_with_common_ids, text_column='text', top_n_words=100)
results['all_data'] = all_data_metrics

save_results_to_file(results, 'whit_neutral_emotion_word_metrics.json')

print("Processing completed.")

"""##stop word"""

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords_from_text(text):
    """
    Function to remove stopwords from the text.
    """
    # Splitting the text into words
    words = text.split()

    # Removing stopwords from the words
    cleaned_words = [word for word in words if word.lower() not in stop_words]  # using lower() to ensure case-insensitivity

    # Joining the words back into text
    return ' '.join(cleaned_words)

# Cleaning the 'text' column by removing stopwords
combined_df['text'] = combined_df['text'].apply(remove_stopwords_from_text)

# Displaying the first 10 rows after stopword removal
print(combined_df['text'].head(10))

"""##top 20 word popleri"""

from collections import Counter
import matplotlib.pyplot as plt

def get_most_common_words(text_series, top_n=20):
    """
    Function to get the most common words from a pandas Series of text.
    """
    # Create a list to hold all words from the text column
    all_words = []

    # Iterate through each text in the series and split into words
    for text in text_series:
        words = text.split()
        all_words.extend(words)  # Add the words to the all_words list

    # Count the frequency of each word using Counter
    word_counts = Counter(all_words)

    # Get the top N most common words
    most_common_words = word_counts.most_common(top_n)

    return most_common_words

# Get the 20 most common words from the cleaned 'text' column
top_20_words = get_most_common_words(combined_df['text'], top_n=20)

# Display the results
print("Top 20 most common words:")
for word, count in top_20_words:
    print(f"{word}: {count}")

# Plot the top 20 words
words, counts = zip(*top_20_words)
plt.figure(figsize=(10, 6))
plt.barh(words, counts, color='skyblue')
plt.xlabel('Frequency')
plt.title('Top 20 Most Common Words in Text')
plt.gca().invert_yaxis()  # Reverse the y-axis to show the most common word on top
plt.show()

# def remove_common_words(text_series, common_words):
#     """
#     Function to remove the most common words from a pandas Series of text.
#     """
#     # Create a set of the most common words to remove
#     common_words_set = set([word for word, _ in common_words])

#     # Define a function to remove the common words from each text
#     def remove_words_from_text(text):
#         words = text.split()
#         filtered_words = [word for word in words if word.lower() not in common_words_set]
#         return " ".join(filtered_words)

#     # Apply this function to the text column and return the cleaned text
#     return text_series.apply(remove_words_from_text)

# # Get the 20 most common words from the cleaned 'text' column
# top_20_words = get_most_common_words(combined_df['text'], top_n=20)

# # Remove the top 20 most common words from the 'text' column
# combined_df['text'] = remove_common_words(combined_df['text'], top_20_words)
from collections import Counter
import pandas as pd

def get_most_common_words(text_series, top_n=20):
    """
    Function to get the most common words in a pandas Series of text.
    """
    # Create a Counter to count word frequencies
    all_words = ' '.join(text_series).split()
    word_counts = Counter([word.lower() for word in all_words])  # Convert to lowercase to ignore case

    # Return the most common words
    return word_counts.most_common(top_n)
def remove_common_words(text_series, common_words):
    """
    Function to remove the most common words from a pandas Series of text.
    """
    # Create a set of the most common words to remove
    common_words_set = set([word for word, _ in common_words])

    # Define a function to remove the common words from each text
    def remove_words_from_text(text):
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in common_words_set]
        return " ".join(filtered_words)

    # Apply this function to the text column and return the cleaned text
    return text_series.apply(remove_words_from_text)

# Get the 20 most common words from the cleaned 'text' column
top_20_words = get_most_common_words(combined_df['text'], top_n=20)

# Remove the top 20 most common words from the 'text' column
combined_df['text'] = remove_common_words(combined_df['text'], top_20_words)

# Display the first 10 rows after removal
print(combined_df['text'].head(10))

"""##Building an emotion dictionary of words and emotion"""

from collections import Counter

def get_unique_words_for_emotions(df, emotion_columns):
    """
    Function to get the most common words for each emotion category in the dataframe.
    """
    emotion_words_dict = {}

    for emotion in emotion_columns:
        # Filter the dataframe for rows where the emotion is 1
        emotion_data = df[df[emotion] == 1]

        # Get the text data for the filtered rows
        texts = emotion_data['text']

        # Get the most common words for the emotion
        most_common_words = get_most_common_words(texts)

        # Add the list of most common words to the dictionary for the emotion
        emotion_words_dict[emotion] = [word for word, _ in most_common_words]

    return emotion_words_dict


# Get the most common words for each emotion
emotion_words = get_unique_words_for_emotions(combined_df, emotion_columns)

# Display the words associated with each emotion
for emotion, words in emotion_words.items():
    print(f"Emotion: {emotion} -> Words: {words}")



import nltk
from nltk.corpus import stopwords
from collections import Counter
import pandas as pd

# Download stopwords if not already downloaded
nltk.download('stopwords', quiet=True)
stop_words = set(stopwords.words('english'))

def clean_text(text_series, top_n_common=20):
    """
    Function to clean text by removing stopwords and the most common words.
    """
    # Step 1: Remove stopwords from the text
    def remove_stopwords(text):
        words = text.split()
        cleaned_words = [word for word in words if word.lower() not in stop_words]
        return ' '.join(cleaned_words)

    # Apply stopword removal to the text series
    text_series = text_series.apply(remove_stopwords)

    # Step 2: Find the most common words after stopword removal
    all_words = ' '.join(text_series).split()
    common_words = Counter([word.lower() for word in all_words]).most_common(top_n_common)
    common_words_set = set([word for word, _ in common_words])

    # Step 3: Remove the most common words from each row of text
    def remove_common_words(text):
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in common_words_set]
        return ' '.join(filtered_words)

    # Apply common word removal to the text series
    return text_series.apply(remove_common_words)

# Clean the 'text' column in the DataFrame
combined_df['text'] = clean_text(combined_df['text'], top_n_common=20)

# Display the first 10 rows after cleaning
print(combined_df['text'].head(10))

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf_vectorizer.fit_transform(combined_df['text'])
X_tfidf

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, combined_df[emotion_columns], test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

model = LogisticRegression(max_iter=1000)
ovr_model = OneVsRestClassifier(model)
ovr_model.fit(X_train, y_train)

from sklearn.metrics import classification_report

y_pred = ovr_model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=emotion_columns))

"""##Logistic Regression model"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report
import pandas as pd

X = combined_df['text']  # The text data
y = combined_df[emotion_columns]  # Emotion columns as target labels (binary for each emotion)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a TfidfVectorizer to convert the text into feature vectors
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)

# Fit and transform the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Create a Logistic Regression model
model = LogisticRegression(max_iter=1000)

# Use OneVsRestClassifier to handle multi-label classification
ovr_model = OneVsRestClassifier(model)

# Train the model on the training data
ovr_model.fit(X_train_tfidf, y_train)

# Make predictions on the test data
y_pred = ovr_model.predict(X_test_tfidf)

# Measure the model's performance
print(classification_report(y_test, y_pred))

# Function to predict the emotion of a new text
def predict_emotion(text):
    # Transform the new text into TF-IDF
    text_tfidf = tfidf_vectorizer.transform([text])
    # Predict the emotion for the new text (multi-label, hence an array of binary results)
    prediction = ovr_model.predict(text_tfidf)
    # Get emotions with a non-zero prediction
    predicted_emotions = [emotion_columns[i] for i in range(len(prediction[0])) if prediction[0][i] == 1]
    return predicted_emotions

# Example prediction
new_text = "I had a fight with a friend who hurt me"
print(f"The predicted emotions for the text are: {predict_emotion(new_text)}")

_pred = ovr_model.predict(X_test_tfidf)

# Print the classification report for model evaluation
print("Classification Report on Test Data:")
print(classification_report(y_test, y_pred))

# Optionally, print predicted emotions for the first few test samples
def display_predictions(X_test, y_test, y_pred, num_samples=10):
    print(f"\nDisplaying predictions for the first {num_samples} test samples:")
    for i in range(num_samples):
        print(f"\nSample {i+1}:")
        print(f"Text: {X_test.iloc[i]}")
        print(f"True Emotions: {y_test.iloc[i].values}")
        predicted_emotions = [emotion_columns[j] for j in range(len(y_pred[i])) if y_pred[i][j] == 1]
        print(f"Predicted Emotions: {predicted_emotions}")

# Displaying the predictions for the first 10 samples in the test set
display_predictions(X_test, y_test, y_pred, num_samples=10)

"""##LSTM"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from sklearn.preprocessing import MultiLabelBinarizer


X = combined_df['text'] 
y = combined_df[emotion_columns] 
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(y.values)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tokenizer = Tokenizer(num_words=5000, lower=True)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_sequence_length = 100 
X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)

model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=100, input_length=max_sequence_length))
model.add(LSTM(units=64, return_sequences=False))
model.add(Dropout(0.5))
model.add(Dense(units=y.shape[1], activation='sigmoid'))  # Sigmoid ליצירת פלט של multi-label
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

history = model.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_data=(X_test_pad, y_test))

score, accuracy = model.evaluate(X_test_pad, y_test)
print(f"Test accuracy: {accuracy * 100:.2f}%")

def predict_emotion_lstm(text):
    text_seq = tokenizer.texts_to_sequences([text])
    text_pad = pad_sequences(text_seq, maxlen=max_sequence_length)
    prediction = model.predict(text_pad)
    predicted_emotions = [emotion_columns[i] for i in range(len(prediction[0])) if prediction[0][i] >= 0.5]
    return predicted_emotions

new_text = "I feel really happy today!"
print(f"The predicted emotions for the text are: {predict_emotion_lstm(new_text)}")

score, accuracy = model.evaluate(X_test_pad, y_test)
print(f"Test accuracy: {accuracy * 100:.2f}%")

from sklearn.metrics import classification_report

y_pred = model.predict(X_test_pad)



y_pred_bin = (y_pred >= 0.5).astype(int)

print(classification_report(y_test, y_pred_bin, target_names=emotion_columns))

classification_report(y_test , y_pred_bin)

"""LLM"""

!pip install transformers torch

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=len(emotion_columns))

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset




X_train, X_test, y_train, y_test = train_test_split(combined_df['text'], combined_df[emotion_columns], test_size=0.2, random_state=42)


def encode_text(texts):

    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')

def create_data_loader(texts, labels, batch_size=16):
    encoded_inputs = encode_text(texts)
    input_ids = encoded_inputs['input_ids']
    attention_mask = encoded_inputs['attention_mask']
    labels = torch.tensor(labels.values, dtype=torch.float32)


    dataset = TensorDataset(input_ids, attention_mask, labels)
    return DataLoader(dataset, batch_size=batch_size)


train_dataloader = create_data_loader(X_train, y_train, batch_size=8)
test_dataloader = create_data_loader(X_test, y_test)

!nvidia-smi

train_dataloader

test_dataloader

from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler

optimizer = AdamW(model.parameters(), lr=2e-5)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

scaler = GradScaler()

def train(model, dataloader, optimizer, epochs=3):
    model.train()
    for epoch in range(epochs):
        for batch in dataloader:
            input_ids = batch[0].to(device)
            attention_mask = batch[1].to(device)
            labels = batch[2].to(device)

            optimizer.zero_grad()

            with autocast(): 
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            print(f'Epoch {epoch+1}, Loss: {loss.item()}')

train(model, train_dataloader, optimizer)

from sklearn.metrics import classification_report

def evaluate_multilabel(model, dataloader, threshold=0.5):
    model.eval()  
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch[0].to(device)
            attention_mask = batch[1].to(device)
            labels = batch[2].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            probs = torch.sigmoid(outputs.logits) 
            preds = (probs > threshold).int()  

            all_preds.append(preds.cpu().numpy())
            all_labels.append(labels.cpu().numpy())

    all_preds = np.vstack(all_preds)
    all_labels = np.vstack(all_labels)

    report = classification_report(all_labels, all_preds, target_names=emotion_columns, zero_division=0)
    return report


print(evaluate_multilabel(model, test_dataloader))

def predict_emotion(text):
    model.eval() 
    encoded_input = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors='pt')  
    with torch.no_grad():
        outputs = model(input_ids=encoded_input['input_ids'].to(device),
                        attention_mask=encoded_input['attention_mask'].to(device))
        preds = torch.argmax(outputs.logits, dim=1)
        predicted_emotions = [emotion_columns[i] for i in preds.cpu().numpy()]
    return predicted_emotions

new_text = "i woke up very tired today. i didn't do much but i think tomorrow might be better"
print(f"The predicted emotions for the text are: {predict_emotion(new_text)}")

import torch.nn.functional as F

def predict_emotion_with_probabilities(text):
    model.eval()  
    encoded_input = tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors='pt')
    with torch.no_grad():
        outputs = model(input_ids=encoded_input['input_ids'].to(device),
                        attention_mask=encoded_input['attention_mask'].to(device))
        probabilities = F.softmax(outputs.logits, dim=1)  # חישוב הסתברויות עם softmax

        emotion_probs = {emotion_columns[i]: prob for i, prob in enumerate(probabilities[0].cpu().numpy())}

    return emotion_probs

# new_text = "i woke up very tired today. i didn't do much but i think tomorrow might be better"
new_text = "i am black"

emotion_probabilities = predict_emotion_with_probabilities(new_text)

print("Emotion probabilities:")
for emotion, prob in emotion_probabilities.items():
    print(f"{emotion}: {prob:.2%}")

import pickle

save_path = "emotion_distilbert_model1.pkl"

with open(save_path, "wb") as f:
    pickle.dump(model.state_dict(), f)

print(f"Model saved as {save_path}")

with open(save_path, "rb") as f:
    loaded_state_dict = pickle.load(f)

model.load_state_dict(loaded_state_dict)

print("Model loaded successfully from pkl file!")

save_path = "emotion_distilbert_model_and_tokenizer1.pkl"

with open(save_path, "wb") as f:
    pickle.dump({
        "model_state_dict": model.state_dict(),
        "tokenizer": tokenizer
    }, f)

print(f"Model and tokenizer saved as {save_path}")

with open(save_path, "rb") as f:
    saved_data = pickle.load(f)

model.load_state_dict(saved_data["model_state_dict"])

tokenizer = saved_data["tokenizer"]

print("Model and tokenizer loaded successfully from pkl file!")

!ls

!ls /path/to/directory

from google.colab import files

files.download('emotion_distilbert_model1.pkl')

from google.colab import drive
drive.mount('/content/drive')

!cp emotion_distilbert_model1.pkl /content/drive/MyDrive/

!cp emotion_distilbert_model_and_tokenizer1.pkl /content/drive/MyDrive/

"""###RoBERTa"""

from transformers import RobertaTokenizer, RobertaForSequenceClassification

model_name = "roberta-base"
tokenizer = RobertaTokenizer.from_pretrained(model_name)
model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=28)

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

def encode_text(texts):
    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')

def create_data_loader(texts, labels, batch_size=16):
    encoded_inputs = encode_text(texts)
    input_ids = encoded_inputs['input_ids']
    attention_mask = encoded_inputs['attention_mask']
    labels = torch.tensor(labels.values, dtype=torch.float32)

    dataset = TensorDataset(input_ids, attention_mask, labels)
    return DataLoader(dataset, batch_size=batch_size)

X_train, X_test, y_train, y_test = train_test_split(combined_df['text'], combined_df[emotion_columns], test_size=0.2, random_state=42)

train_dataloader = create_data_loader(X_train, y_train, batch_size=4)
test_dataloader = create_data_loader(X_test, y_test, batch_size=4)

from transformers import RobertaTokenizer, RobertaForSequenceClassification
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import torch
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler



model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=len(emotion_columns))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

scaler = GradScaler()

accumulation_steps = 4  

def train(model, dataloader, optimizer, epochs=3):
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad() 
        for i, batch in enumerate(dataloader):
            input_ids = batch[0].to(device)  
            attention_mask = batch[1].to(device)  
            labels = batch[2].to(device)  

            with autocast():
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss

            scaler.scale(loss).backward()

            if (i + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad() 

            print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {loss.item()}')

        torch.cuda.empty_cache()  
train(model, train_dataloader, optimizer)

"""###ALBERT"""

from transformers import AlbertTokenizer, AlbertForSequenceClassification

model_name = "albert-base-v2"
tokenizer = AlbertTokenizer.from_pretrained(model_name)
model = AlbertForSequenceClassification.from_pretrained(model_name)

"""###XLNet"""

from transformers import XLNetTokenizer, XLNetForSequenceClassification

model_name = "xlnet-base-cased"
tokenizer = XLNetTokenizer.from_pretrained(model_name)
model = XLNetForSequenceClassification.from_pretrained(model_name)

"""###T5 (Text-to-Text Transfer Transformer)"""

from transformers import T5Tokenizer, T5ForConditionalGeneration

model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

"""###GPT-2"""

from transformers import GPT2Tokenizer, GPT2LMHeadModel

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

"""###ELECTRA"""

from transformers import ElectraTokenizer, ElectraForSequenceClassification

model_name = "electra-base-discriminator"
tokenizer = ElectraTokenizer.from_pretrained(model_name)
model = ElectraForSequenceClassification.from_pretrained(model_name)

"""### BART (Bidirectional and Auto-Regressive Transformers)

"""

from transformers import BartTokenizer, BartForSequenceClassification

model_name = "facebook/bart-base"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForSequenceClassification.from_pretrained(model_name)

"""### DeBERTa (Decoding-enhanced BERT with disentangled attention)

"""

from transformers import DebertaTokenizer, DebertaForSequenceClassification

model_name = "microsoft/deberta-base"
tokenizer = DebertaTokenizer.from_pretrained(model_name)
model = DebertaForSequenceClassification.from_pretrained(model_name)

"""###BERTweet"""

from transformers import BertTokenizer, BertForSequenceClassification

model_name = "vinai/bertweet-base"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name)
