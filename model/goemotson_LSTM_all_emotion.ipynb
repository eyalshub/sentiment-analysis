{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRLOyoaMAEFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a0021b-53a1-4b48-e992-3333ef35d5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (3.1.5)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pyLDAvis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.2)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n",
            "--2025-02-25 12:05:12--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.38, 13.35.7.82, 13.35.7.50, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G   139MB/s    in 34s     \n",
            "\n",
            "2025-02-25 12:05:47 (126 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313471 sha256=faede2f84e2209684ff91b804016eed84b8656c04640f0bd61c3a277edaaf0be\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard\n",
        "!pip install gensim\n",
        "!pip install pyLDAvis\n",
        "!pip install wordcloud matplotlib\n",
        "!pip install emoji\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "!gzip -d cc.en.300.bin.gz\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict, Counter\n",
        "import pandas as pd\n",
        "import emoji\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "vl_JMw_qAPzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0062769c-8b16-41d3-9b1f-2e015ad05d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
      ],
      "metadata": {
        "id": "JZb4ySItASYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14709366-6ab7-4691-e54d-f2c5f5726222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-25 12:07:41--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.207, 142.250.157.207, 142.251.8.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14174600 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_1.csv’\n",
            "\n",
            "goemotions_1.csv    100%[===================>]  13.52M  8.32MB/s    in 1.6s    \n",
            "\n",
            "2025-02-25 12:07:43 (8.32 MB/s) - ‘data/full_dataset/goemotions_1.csv’ saved [14174600/14174600]\n",
            "\n",
            "--2025-02-25 12:07:43--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.207, 142.250.157.207, 142.251.8.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14173154 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_2.csv’\n",
            "\n",
            "goemotions_2.csv    100%[===================>]  13.52M  9.43MB/s    in 1.4s    \n",
            "\n",
            "2025-02-25 12:07:44 (9.43 MB/s) - ‘data/full_dataset/goemotions_2.csv’ saved [14173154/14173154]\n",
            "\n",
            "--2025-02-25 12:07:45--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.207, 142.250.157.207, 142.251.8.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14395164 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_3.csv’\n",
            "\n",
            "goemotions_3.csv    100%[===================>]  13.73M  8.35MB/s    in 1.6s    \n",
            "\n",
            "2025-02-25 12:07:46 (8.35 MB/s) - ‘data/full_dataset/goemotions_3.csv’ saved [14395164/14395164]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "goemotions_1 = pd.read_csv('data/full_dataset/goemotions_1.csv')\n",
        "goemotions_2 = pd.read_csv('data/full_dataset/goemotions_2.csv')\n",
        "goemotions_3 = pd.read_csv('data/full_dataset/goemotions_3.csv')\n",
        "\n",
        "combined_df = pd.concat([goemotions_1, goemotions_2, goemotions_3], ignore_index=True)\n",
        "emotion_columns = combined_df.columns[9:]\n",
        "\n",
        "combined_df['created_utc'] = pd.to_datetime(combined_df['created_utc'], unit='s', errors='coerce')\n",
        "combined_df.to_csv('data/full_dataset/goemotions_combined.csv', index=False)\n",
        "combined_df = combined_df.query(\"example_very_unclear != True\")"
      ],
      "metadata": {
        "id": "4dYrzKlyAT6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#work white emoji..."
      ],
      "metadata": {
        "id": "wFGv5d80AWiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dictionary to store emojis with their positions and texts\n",
        "emoji_dict = {}\n",
        "\n",
        "# Function to extract emojis and build the dictionary\n",
        "def build_emoji_dict(df):\n",
        "    for idx, row in df.iterrows():\n",
        "        text = row['text']\n",
        "        for char in text:\n",
        "            if emoji.is_emoji(char):\n",
        "                if char not in emoji_dict:\n",
        "                    emoji_dict[char] = []\n",
        "                emoji_dict[char].append((idx, text))\n",
        "build_emoji_dict(combined_df)\n"
      ],
      "metadata": {
        "id": "U5yorT0uAZ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process"
      ],
      "metadata": {
        "id": "7sT_g1m9AiMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"[<>]\", \"\", text)\n",
        "    text = f\"[START] {text} [END]\"\n",
        "\n",
        "    text = re.sub(r'\\[(\\w+)\\]', r'<\\1>', text)\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9<> ]\", \"\", text)\n",
        "\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "combined_df['processed_text'] = combined_df['text'].apply(preprocess_text)\n",
        "\n",
        "print(combined_df[['text', 'processed_text']].head())"
      ],
      "metadata": {
        "id": "Sjt1V2V2Akgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30eb6fe-e275-4057-ff79-d39914924a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0                                    That game hurt.   \n",
            "2     You do right, if you don't care then fuck 'em!   \n",
            "3                                 Man I love reddit.   \n",
            "4  [NAME] was nowhere near them, he was by the Fa...   \n",
            "5  Right? Considering it’s such an important docu...   \n",
            "\n",
            "                                      processed_text  \n",
            "0                       <start> that game hurt <end>  \n",
            "2  <start> you do right if you dont care then fuc...  \n",
            "3                    <start> man i love reddit <end>  \n",
            "4  <start> <name> was nowhere near them he was by...  \n",
            "5  <start> right considering its such an importan...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mappings(df):\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "    index_counter = 1\n",
        "\n",
        "    for text in df['processed_text']:\n",
        "        words = text.split()\n",
        "\n",
        "        for word in words:\n",
        "            if word not in word_to_index:\n",
        "                word_to_index[word] = index_counter\n",
        "                index_to_word[index_counter] = word\n",
        "                index_counter += 1\n",
        "\n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "\n",
        "word_to_index, index_to_word = create_mappings(combined_df)\n",
        "\n",
        "print(\"Word to Index Mapping:\", dict(list(word_to_index.items())[:10]))\n",
        "print(\"Index to Word Mapping:\", dict(list(index_to_word.items())[:10]))"
      ],
      "metadata": {
        "id": "yt17tYfnAo3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96df8c1e-eca3-425a-a579-76e0f49df55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word to Index Mapping: {'<start>': 1, 'that': 2, 'game': 3, 'hurt': 4, '<end>': 5, 'you': 6, 'do': 7, 'right': 8, 'if': 9, 'dont': 10}\n",
            "Index to Word Mapping: {1: '<start>', 2: 'that', 3: 'game', 4: 'hurt', 5: '<end>', 6: 'you', 7: 'do', 8: 'right', 9: 'if', 10: 'dont'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_to_index),len(index_to_word)"
      ],
      "metadata": {
        "id": "zWcMuJzbAtoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ff28cf-7934-4ce1-dfaa-db87ca01f26b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33701, 33701)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_index_sequence(text, word_to_index):\n",
        "    words = text.split()\n",
        "    index_sequence = [word_to_index[word] for word in words if word in word_to_index]\n",
        "    return index_sequence\n",
        "combined_df['text_as_indexes'] = combined_df['processed_text'].apply(lambda x: text_to_index_sequence(x, word_to_index))\n",
        "print(combined_df[['text', 'processed_text', 'text_as_indexes']].head())"
      ],
      "metadata": {
        "id": "68zDtA01AvJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b5cbff-eda2-4fab-eecc-59062e107ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0                                    That game hurt.   \n",
            "2     You do right, if you don't care then fuck 'em!   \n",
            "3                                 Man I love reddit.   \n",
            "4  [NAME] was nowhere near them, he was by the Fa...   \n",
            "5  Right? Considering it’s such an important docu...   \n",
            "\n",
            "                                      processed_text  \\\n",
            "0                       <start> that game hurt <end>   \n",
            "2  <start> you do right if you dont care then fuc...   \n",
            "3                    <start> man i love reddit <end>   \n",
            "4  <start> <name> was nowhere near them he was by...   \n",
            "5  <start> right considering its such an importan...   \n",
            "\n",
            "                                     text_as_indexes  \n",
            "0                                    [1, 2, 3, 4, 5]  \n",
            "2          [1, 6, 7, 8, 9, 6, 10, 11, 12, 13, 14, 5]  \n",
            "3                             [1, 15, 16, 17, 18, 5]  \n",
            "4     [1, 19, 20, 21, 22, 23, 24, 20, 25, 26, 27, 5]  \n",
            "5  [1, 8, 28, 29, 30, 31, 32, 33, 16, 34, 35, 26,...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#word2Vec"
      ],
      "metadata": {
        "id": "YysTC3V9AzZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_word2vec(df):\n",
        "    processed_texts = df['processed_text'].apply(lambda x: x.split())\n",
        "    return processed_texts\n",
        "\n",
        "sentences = prepare_data_for_word2vec(combined_df)\n",
        "model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
        "word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
        "print(\"Word vector for 'start':\", word_vectors['start'])"
      ],
      "metadata": {
        "id": "95Kj5GgwA1WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FestText"
      ],
      "metadata": {
        "id": "BLKSIvWjA3dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "fasttext_model = fasttext.load_model('/content/cc.en.300.bin')\n",
        "\n",
        "def prepare_data_for_fasttext(df):\n",
        "    processed_texts = df['processed_text'].apply(lambda x: x.split())\n",
        "    return processed_texts\n",
        "\n",
        "sentences = prepare_data_for_fasttext(combined_df)\n",
        "\n",
        "word_vectors = {word: fasttext_model.get_word_vector(word) for sentence in sentences for word in sentence}\n",
        "\n",
        "print(\"Word vector for 'start':\", word_vectors.get('start', 'Word not found'))\n"
      ],
      "metadata": {
        "id": "FsmIXkupAyay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f04a002-2605-44ca-835a-fdb67a2c0464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'start': [ 7.91064836e-03 -9.43343155e-03  2.28888560e-02  1.15760624e-01\n",
            " -5.44784889e-02  3.52381244e-02  1.51470797e-02  9.68480669e-03\n",
            "  3.72007266e-02 -8.26112553e-03 -3.63847502e-02  7.02371597e-02\n",
            " -1.53566375e-02  1.11012265e-01  1.33964136e-01  6.41972050e-02\n",
            "  6.07942492e-02 -1.30296145e-02  1.28229149e-04  9.35186446e-02\n",
            " -6.10206239e-02 -2.02715285e-02 -1.50931003e-02 -9.10209268e-02\n",
            "  1.57276485e-02 -1.85630284e-02 -6.97859824e-02  6.44239336e-02\n",
            " -4.34500724e-02  3.08100097e-02  2.29679141e-03 -5.58396354e-02\n",
            " -6.58368096e-02 -6.34431988e-02  3.13772000e-02  2.18865033e-02\n",
            "  3.18698958e-02  1.05855837e-01 -2.08669528e-03 -9.21282396e-02\n",
            "  3.49404141e-02 -9.22180153e-03  8.44760537e-02  6.59895763e-02\n",
            " -3.71814519e-02  7.32254684e-02  3.71225476e-02 -6.29411414e-02\n",
            " -3.49417143e-02 -3.72454687e-03  1.67517923e-02  1.74221359e-02\n",
            "  8.67096111e-02 -5.21334670e-02 -9.95707512e-03 -1.62409935e-02\n",
            "  3.27702239e-02  3.58919427e-03 -8.23421478e-02 -1.64883211e-03\n",
            "  2.35430859e-02 -1.95483044e-02 -1.48018319e-02 -3.93047668e-02\n",
            "  2.78750677e-02  1.08272210e-02 -3.63175757e-02  2.45057344e-02\n",
            " -2.45150141e-02 -1.99477486e-02 -4.50449921e-02  5.69998957e-02\n",
            "  7.89616108e-02 -3.90553698e-02  3.14676613e-02  1.68070756e-02\n",
            "  4.17714380e-02  5.23959398e-02 -5.52481711e-02 -1.10786021e-01\n",
            " -2.96174120e-02 -3.86502780e-02 -4.10447381e-02  1.72210820e-02\n",
            "  6.14874698e-02 -8.61671288e-03 -2.69972570e-02 -3.49278860e-02\n",
            "  2.14060955e-02  6.54201508e-02  3.75608541e-02  8.62294622e-03\n",
            "  4.10126820e-02  3.75329480e-02 -4.54161316e-04 -7.46523589e-02\n",
            "  1.19725689e-02  8.63861069e-02 -1.56153310e-02 -3.10133281e-03\n",
            "  1.80034544e-02 -2.69964826e-03  2.36624274e-02 -1.77627653e-02\n",
            " -9.99894924e-03 -6.89931363e-02  5.49511947e-02 -8.18606745e-03\n",
            " -5.50479814e-02  5.67293055e-02  6.33748695e-02  3.78584117e-02\n",
            "  2.46352162e-02  6.17731363e-02  3.78466137e-02 -3.32530960e-02\n",
            " -1.30310711e-02  1.43337231e-02 -1.47374831e-02 -3.63188274e-02\n",
            "  1.19018769e-02 -1.16963871e-02 -5.23666590e-02  1.23278908e-02\n",
            " -3.40386033e-02  3.73916626e-02  2.44609006e-02 -5.71385324e-02\n",
            " -4.51593921e-02  3.68556678e-02  5.23006590e-03 -5.34112900e-02\n",
            " -7.49978656e-03 -4.40601446e-03 -7.52404891e-03 -2.76714079e-02\n",
            "  2.05516443e-03  4.17595878e-02 -1.59559529e-02  2.10030973e-02\n",
            " -3.47240642e-03 -9.44499485e-03 -8.22217837e-02  1.18041234e-02\n",
            "  3.88502777e-02 -1.99154858e-02 -1.32566810e-01 -1.15718339e-02\n",
            " -4.02939059e-02  1.99113935e-02 -2.49291994e-02  3.17915864e-02\n",
            "  5.21808453e-02  1.63629875e-02 -1.80177577e-02  4.20684479e-02\n",
            "  8.96415636e-02  3.64128873e-02  1.74677521e-02  1.05631840e-03\n",
            "  8.97897258e-02 -2.97745094e-02 -2.76565850e-02  6.59949798e-03\n",
            "  8.95437300e-02  4.94447537e-02  5.44326380e-02  5.68558648e-02\n",
            "  1.07445613e-01 -3.20494957e-02  1.88628994e-02 -3.72604206e-02\n",
            "  2.14162050e-03 -3.38861533e-02 -9.13295597e-02  5.71936974e-03\n",
            " -2.35068798e-02 -8.15444440e-02  5.17817624e-02  3.18397544e-02\n",
            " -9.82184429e-03  3.74551788e-02 -1.24099359e-01 -2.72542518e-02\n",
            "  1.72981191e-02 -1.76331811e-02 -6.66407123e-02  5.00974990e-02\n",
            " -4.69275042e-02 -8.53030756e-03 -4.68208715e-02  2.12671552e-02\n",
            "  4.21970487e-02  2.79996879e-02  5.53477276e-03  3.60253826e-02\n",
            "  1.63910333e-02 -3.07853781e-02 -2.90625580e-02  4.04237807e-02\n",
            " -4.98710871e-02  5.46280742e-02  7.08662719e-02 -3.33852619e-02\n",
            " -2.75859237e-03  1.33099724e-02  2.02648342e-03 -5.01098856e-02\n",
            "  2.45747324e-02  5.15799634e-02  1.78679917e-02 -6.27648979e-02\n",
            "  6.29340038e-02  1.86706558e-02  1.22492518e-02 -4.28927541e-02\n",
            " -6.07561832e-03 -1.98474545e-02 -6.50100410e-05 -9.90893319e-03\n",
            " -5.89036755e-03  2.44298019e-02  2.87071709e-02  8.89896676e-02\n",
            "  2.32130997e-02  2.09821835e-02 -1.16553633e-02 -2.56799851e-02\n",
            "  1.49275158e-02 -1.41543886e-02 -2.55170390e-02  2.21941993e-02\n",
            " -7.35114813e-02 -2.99032833e-02 -2.75981668e-02  3.32949385e-02\n",
            " -6.08316064e-02 -3.97252068e-02 -1.55551406e-02 -1.57969203e-02\n",
            " -5.52369654e-03 -2.69691944e-02 -1.04175881e-01 -7.95569196e-02\n",
            "  1.48095250e-01 -1.88155975e-02  3.76277044e-02  4.35854793e-02\n",
            "  3.75146754e-02 -1.28051132e-01  4.25304696e-02  4.62437347e-02\n",
            "  8.02811980e-02 -6.56051040e-02 -2.88130995e-02  7.91168399e-03\n",
            " -2.23641302e-02  2.61326954e-02  1.06501400e-01  9.86915082e-04\n",
            " -3.44330594e-02  1.84895354e-03 -1.41664147e-02  1.26424832e-02\n",
            " -3.42138037e-02  3.41970511e-02  4.34703566e-03 -1.09482929e-03\n",
            " -4.76906449e-02 -5.05028442e-02 -6.99414164e-02 -2.49411706e-02\n",
            " -2.77189538e-02 -3.19655128e-02  2.27888916e-02  5.49681857e-03\n",
            " -1.19039090e-02  4.84818779e-02  2.84778550e-02  3.67808267e-02\n",
            "  6.28819466e-02  7.00776577e-02 -9.78844836e-02 -1.67301893e-02\n",
            " -3.13598216e-02  9.44845937e-03  2.81970240e-02 -7.86999762e-02\n",
            " -1.07762162e-02 -5.06495796e-02  8.36703107e-02 -2.82265749e-02\n",
            " -4.29264903e-02  1.19034667e-03 -9.30781439e-02 -9.23760980e-03\n",
            "  1.51935816e-02  7.12248031e-03 -1.30619612e-02 -4.24892642e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#work white the labell"
      ],
      "metadata": {
        "id": "1ZpYFPpoBOPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_data_for_emotion(combined_df, emotion_columns, emotion, test_size=0.2, random_state=42):\n",
        "    # Separate the entries for the specific emotion\n",
        "    emotion_entries = combined_df[combined_df[emotion] == 1].copy()\n",
        "\n",
        "    # Set other emotion columns to 0\n",
        "    for col in emotion_columns:\n",
        "        if col != emotion:\n",
        "            emotion_entries[col] = 0\n",
        "\n",
        "    # Combine non-specific emotion entries\n",
        "    all_other_emotions = pd.concat([combined_df[combined_df[e] == 1] for e in emotion_columns if e != emotion])\n",
        "\n",
        "    # Handle potential list-type columns\n",
        "    for col in all_other_emotions.columns:\n",
        "        if all_other_emotions[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            all_other_emotions[col] = all_other_emotions[col].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
        "\n",
        "    # Remove duplicates\n",
        "    all_other_emotions = all_other_emotions.drop_duplicates()\n",
        "\n",
        "    # Split data into train and test\n",
        "    train_emotion, test_emotion = train_test_split(emotion_entries, test_size=test_size, random_state=random_state)\n",
        "    train_other, test_other = train_test_split(all_other_emotions, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Filter to keep only processed_text and current emotion\n",
        "    train_emotion_filtered = train_emotion[['processed_text', emotion]]\n",
        "    test_emotion_filtered = test_emotion[['processed_text', emotion]]\n",
        "    train_other_filtered = train_other[['processed_text', emotion]]\n",
        "    test_other_filtered = test_other[['processed_text', emotion]]\n",
        "\n",
        "    # Combine emotion and non-emotion entries\n",
        "    train_combined = pd.concat([train_emotion_filtered, train_other_filtered], ignore_index=True)\n",
        "    test_combined = pd.concat([test_emotion_filtered, test_other_filtered], ignore_index=True)\n",
        "\n",
        "    print(f\"{emotion} - Train: {len(train_combined)}, Test: {len(test_combined)}\")\n",
        "\n",
        "    return train_combined, test_combined\n",
        "\n",
        "# Apply for all emotions\n",
        "emotion_data = {}\n",
        "for emotion in emotion_columns:\n",
        "    if emotion in combined_df.columns:\n",
        "        train, test = split_data_for_emotion(combined_df, emotion_columns, emotion)\n",
        "        emotion_data[emotion] = {'train': train, 'test': test}\n",
        "    else:\n",
        "        print(f\"Emotion {emotion} not found in the dataset.\")\n",
        "\n",
        "print(\"Data splitting complete for all emotions!\")\n"
      ],
      "metadata": {
        "id": "10ZeKI5FCWJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e616ad-d3ea-42b1-f423-21066dd58287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "admiration - Train: 171530, Test: 42884\n",
            "amusement - Train: 168743, Test: 42186\n",
            "anger - Train: 168556, Test: 42140\n",
            "annoyance - Train: 170471, Test: 42619\n",
            "approval - Train: 171340, Test: 42835\n",
            "caring - Train: 168231, Test: 42059\n",
            "confusion - Train: 168187, Test: 42048\n",
            "curiosity - Train: 169296, Test: 42325\n",
            "desire - Train: 167586, Test: 41898\n",
            "disappointment - Train: 169261, Test: 42316\n",
            "disapproval - Train: 169241, Test: 42311\n",
            "disgust - Train: 168160, Test: 42041\n",
            "embarrassment - Train: 167084, Test: 41773\n",
            "excitement - Train: 168338, Test: 42085\n",
            "fear - Train: 167385, Test: 41848\n",
            "gratitude - Train: 169891, Test: 42473\n",
            "grief - Train: 166508, Test: 41628\n",
            "joy - Train: 169174, Test: 42294\n",
            "love - Train: 168837, Test: 42211\n",
            "nervousness - Train: 167062, Test: 41766\n",
            "optimism - Train: 169608, Test: 42402\n",
            "pride - Train: 166740, Test: 41686\n",
            "realization - Train: 169508, Test: 42377\n",
            "relief - Train: 166651, Test: 41664\n",
            "remorse - Train: 167063, Test: 41766\n",
            "sadness - Train: 168595, Test: 42150\n",
            "surprise - Train: 167884, Test: 41972\n",
            "neutral - Train: 166250, Test: 41564\n",
            "Data splitting complete for all emotions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ffFJ6e4DfP-",
        "outputId": "f885240c-cf6c-4897-f670-d91a3ac68c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the data for input into the model"
      ],
      "metadata": {
        "id": "DDLIUZxwDnSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word_vectors, max_len=100):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word_vectors = word_vectors\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        emotion_label = self.labels[idx]\n",
        "\n",
        "        indexed_text = [self.word_vectors.get(word, np.zeros(300)) for word in text.split()]\n",
        "\n",
        "        indexed_text = indexed_text[:self.max_len] if len(indexed_text) > self.max_len else indexed_text + [np.zeros(300)] * (self.max_len - len(indexed_text))\n",
        "\n",
        "        indexed_text = torch.tensor(np.array(indexed_text), dtype=torch.float)\n",
        "        emotion_label = torch.tensor(emotion_label, dtype=torch.float)\n",
        "\n",
        "        return indexed_text, emotion_label\n",
        "\n",
        "# Dictionary to store dataloaders for each emotion\n",
        "emotion_dataloaders = {}\n",
        "\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "for emotion in emotion_columns:\n",
        "    train_texts = emotion_data[emotion]['train']['processed_text'].tolist()\n",
        "    train_labels = emotion_data[emotion]['train'][emotion].tolist()\n",
        "\n",
        "    test_texts = emotion_data[emotion]['test']['processed_text'].tolist()\n",
        "    test_labels = emotion_data[emotion]['test'][emotion].tolist()\n",
        "\n",
        "    test_dataset = EmotionDataset(test_texts, test_labels, word_vectors)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    emotion_dataloaders[emotion] = {'test': test_dataloader}\n",
        "\n",
        "    X = np.array(train_texts)\n",
        "    y = np.array(train_labels)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        print(f\"{emotion} - Training on Fold {fold+1}...\")\n",
        "\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        train_dataset_fold = EmotionDataset(X_train_fold, y_train_fold, word_vectors)\n",
        "        val_dataset_fold = EmotionDataset(X_val_fold, y_val_fold, word_vectors)\n",
        "\n",
        "        train_dataloader_fold = DataLoader(train_dataset_fold, batch_size=32, shuffle=True)\n",
        "        val_dataloader_fold = DataLoader(val_dataset_fold, batch_size=32, shuffle=False)\n",
        "\n",
        "        if 'folds' not in emotion_dataloaders[emotion]:\n",
        "            emotion_dataloaders[emotion]['folds'] = []\n",
        "\n",
        "        emotion_dataloaders[emotion]['folds'].append({'train': train_dataloader_fold, 'val': val_dataloader_fold})\n",
        "\n",
        "print(\"Dataloaders ready for all emotions!\")\n"
      ],
      "metadata": {
        "id": "BotRH0bDDwRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47d4232-7043-4427-f396-0440f3df460e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "admiration - Training on Fold 1...\n",
            "admiration - Training on Fold 2...\n",
            "admiration - Training on Fold 3...\n",
            "admiration - Training on Fold 4...\n",
            "admiration - Training on Fold 5...\n",
            "amusement - Training on Fold 1...\n",
            "amusement - Training on Fold 2...\n",
            "amusement - Training on Fold 3...\n",
            "amusement - Training on Fold 4...\n",
            "amusement - Training on Fold 5...\n",
            "anger - Training on Fold 1...\n",
            "anger - Training on Fold 2...\n",
            "anger - Training on Fold 3...\n",
            "anger - Training on Fold 4...\n",
            "anger - Training on Fold 5...\n",
            "annoyance - Training on Fold 1...\n",
            "annoyance - Training on Fold 2...\n",
            "annoyance - Training on Fold 3...\n",
            "annoyance - Training on Fold 4...\n",
            "annoyance - Training on Fold 5...\n",
            "approval - Training on Fold 1...\n",
            "approval - Training on Fold 2...\n",
            "approval - Training on Fold 3...\n",
            "approval - Training on Fold 4...\n",
            "approval - Training on Fold 5...\n",
            "caring - Training on Fold 1...\n",
            "caring - Training on Fold 2...\n",
            "caring - Training on Fold 3...\n",
            "caring - Training on Fold 4...\n",
            "caring - Training on Fold 5...\n",
            "confusion - Training on Fold 1...\n",
            "confusion - Training on Fold 2...\n",
            "confusion - Training on Fold 3...\n",
            "confusion - Training on Fold 4...\n",
            "confusion - Training on Fold 5...\n",
            "curiosity - Training on Fold 1...\n",
            "curiosity - Training on Fold 2...\n",
            "curiosity - Training on Fold 3...\n",
            "curiosity - Training on Fold 4...\n",
            "curiosity - Training on Fold 5...\n",
            "desire - Training on Fold 1...\n",
            "desire - Training on Fold 2...\n",
            "desire - Training on Fold 3...\n",
            "desire - Training on Fold 4...\n",
            "desire - Training on Fold 5...\n",
            "disappointment - Training on Fold 1...\n",
            "disappointment - Training on Fold 2...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EmotionLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim=300, hidden_dim=128, output_dim=1, n_layers=2, dropout=0.5):\n",
        "        super(EmotionLSTMWithAttention, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(dropout)\n",
        "        # Bidirectional LSTM Layer\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)  # Attention mechanism to calculate attention scores\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        # LayerNorm after LSTM\n",
        "        self.ln = nn.LayerNorm(hidden_dim * 2)  # Since bidirectional, we multiply hidden_dim by 2\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "        # Dropout after the LSTM and fully connected layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def attention_mechanism(self, lstm_output):\n",
        "        \"\"\"\n",
        "        Applies attention mechanism over the LSTM outputs.\n",
        "        \"\"\"\n",
        "        attention_scores = self.attention(lstm_output)  # Shape: (batch_size, seq_len, 1)\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)  # Shape: (batch_size, seq_len, 1)\n",
        "        weighted_sum = torch.sum(attention_weights * lstm_output, dim=1)  # Shape: (batch_size, hidden_dim*2)\n",
        "        weighted_sum = self.attn_dropout(weighted_sum)\n",
        "        return weighted_sum\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is the input tensor of shape (batch_size, seq_len, input_dim)\n",
        "        x = self.input_dropout(x)\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_out = self.attention_mechanism(lstm_out)\n",
        "\n",
        "        # Layer normalization\n",
        "        attention_out = self.ln(attention_out)\n",
        "\n",
        "        # Pass through fully connected layer and dropout\n",
        "        out = self.fc(attention_out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Instantiate the model with the desired parameters\n",
        "emotion_models = {}\n",
        "\n",
        "for emotion in emotion_columns:\n",
        "    model = EmotionLSTMWithAttention(input_dim=300, hidden_dim=128, output_dim=1, n_layers=2, dropout=0.5)\n",
        "    emotion_models[emotion] = model"
      ],
      "metadata": {
        "id": "Hh76ygQBBcFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3_xMNpb4FBiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Focal Loss Implementation\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, reduction='mean', pos_weight=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight, reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return F_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return F_loss.sum()\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "# Training function with additional metrics, early stopping, and LR scheduler\n",
        "def simple_train(model, train_dataloader_fold, val_dataloader_fold, criterion, optimizer, epochs=5):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    val_precisions = []\n",
        "    val_recalls = []\n",
        "    val_f1s = []\n",
        "    val_aucs = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "        epoch_train_acc = 0\n",
        "\n",
        "        for inputs, labels in train_dataloader_fold:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            labels = labels.float()\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            preds = torch.round(torch.sigmoid(outputs)).detach().cpu().numpy()\n",
        "            batch_acc = f1_score(labels.detach().cpu().numpy(), preds, average=\"macro\")\n",
        "            epoch_train_acc += batch_acc\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / len(train_dataloader_fold)\n",
        "        avg_train_acc = epoch_train_acc / len(train_dataloader_fold)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuracies.append(avg_train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0\n",
        "        epoch_val_acc = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_dataloader_fold:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                labels = labels.float()\n",
        "                loss = criterion(outputs.squeeze(), labels)\n",
        "                epoch_val_loss += loss.item()\n",
        "\n",
        "                preds = torch.round(torch.sigmoid(outputs)).detach().cpu().numpy()\n",
        "                batch_acc = f1_score(labels.detach().cpu().numpy(), preds, average=\"macro\")\n",
        "                epoch_val_acc += batch_acc\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        avg_val_loss = epoch_val_loss / len(val_dataloader_fold)\n",
        "        avg_val_acc = epoch_val_acc / len(val_dataloader_fold)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(avg_val_acc)\n",
        "\n",
        "        epoch_val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "        epoch_val_precision = precision_score(all_labels, all_preds, average='macro')\n",
        "        epoch_val_recall = recall_score(all_labels, all_preds, average='macro')\n",
        "        try:\n",
        "            epoch_val_auc = roc_auc_score(all_labels, all_preds)\n",
        "        except Exception:\n",
        "            epoch_val_auc = 0.0\n",
        "\n",
        "        val_f1s.append(epoch_val_f1)\n",
        "        val_precisions.append(epoch_val_precision)\n",
        "        val_recalls.append(epoch_val_recall)\n",
        "        val_aucs.append(epoch_val_auc)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), f'best_model_{label}.pt')\n",
        "            print(\"  Saved the best model!\")\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"  Early stopping triggered. Stopping training.\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, val_precisions, val_recalls, val_f1s, val_aucs\n",
        "\n",
        "# Training loop for all emotion models\n",
        "for label, model in emotion_models.items():\n",
        "    print(f\"Starting training for emotion: {label}\")\n",
        "\n",
        "    train_labels = train_combined[label].tolist()\n",
        "    val_labels = val_combined[label].tolist()\n",
        "\n",
        "    label_counts = np.bincount(train_labels)\n",
        "    weights = 1.0 / label_counts\n",
        "    sample_weights = weights[train_labels]\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    pos_count = np.sum(train_labels)\n",
        "    neg_count = len(train_labels) - pos_count\n",
        "    pos_weight = torch.tensor([neg_count / pos_count]).to(device)\n",
        "\n",
        "    criterion = FocalLoss(alpha=0.20, gamma=2.5, pos_weight=pos_weight)\n",
        "\n",
        "    train_dataset = EmotionDataset(train_combined['processed_text'].tolist(), train_labels, word_vectors)\n",
        "    val_dataset = EmotionDataset(val_combined['processed_text'].tolist(), val_labels, word_vectors)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "\n",
        "    results = simple_train(model, train_dataloader, val_dataloader, criterion, optimizer, epochs=20)\n"
      ],
      "metadata": {
        "id": "eNeTQyH1FLBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test for each model"
      ],
      "metadata": {
        "id": "K_H5hWlgGFCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(log_dir='./runs/evaluation_results')\n",
        "\n",
        "# Evaluate test metrics function\n",
        "def evaluate_test_metrics(model, label_name, test_dataloader, criterion):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_outputs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_outputs = np.array(all_outputs)\n",
        "\n",
        "    # Calculating metrics\n",
        "    f1 = f1_score(all_labels, (all_outputs > 0.4).astype(int))\n",
        "    roc_auc = roc_auc_score(all_labels, all_outputs)\n",
        "    accuracy = np.mean((all_labels == (all_outputs > 0.4).astype(int)))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, (all_outputs > 0.4).astype(int))\n",
        "\n",
        "    # Precision and recall\n",
        "    precision = precision_score(all_labels, (all_outputs > 0.7).astype(int))\n",
        "    recall = recall_score(all_labels, (all_outputs > 0.3).astype(int))\n",
        "\n",
        "    # Specificity\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "\n",
        "    # Printing results\n",
        "    print(f\"Label: {label_name}\")\n",
        "    print(f\"Test F1 Score: {f1:.4f}\")\n",
        "    print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test Specificity: {specificity:.4f}\\n\")\n",
        "\n",
        "    # Logging results in TensorBoard\n",
        "    writer.add_scalar(f'{label_name}/Test F1 Score', f1)\n",
        "    writer.add_scalar(f'{label_name}/Test ROC AUC', roc_auc)\n",
        "    writer.add_scalar(f'{label_name}/Test Accuracy', accuracy)\n",
        "    writer.add_scalar(f'{label_name}/Test Precision', precision)\n",
        "    writer.add_scalar(f'{label_name}/Test Recall', recall)\n",
        "    writer.add_scalar(f'{label_name}/Test Specificity', specificity)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(f\"Confusion Matrix for {label_name}\")\n",
        "    plt.show()\n",
        "\n",
        "# Dictionary of models per label\n",
        "label_models = {\n",
        "    'joy': joy_model,\n",
        "    'sadness': sadness_model,\n",
        "    'anger': anger_model,\n",
        "    'fear': fear_model\n",
        "}\n",
        "\n",
        "# Evaluating each model\n",
        "for label, model in label_models.items():\n",
        "    evaluate_test_metrics(model, label, test_dataloader, criterion)\n",
        "\n",
        "# Closing the TensorBoard writer\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "7zxoqnOmGDst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#the score"
      ],
      "metadata": {
        "id": "tgkMYn77IkOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Function to get the probabilities for a new text\n",
        "def get_emotion_probabilities(text, emotion_models):\n",
        "    probabilities = {}\n",
        "\n",
        "    for label, model in emotion_models.items():\n",
        "        model.eval()\n",
        "        # Assuming 'tokens' is the representation of the text as tokens (already preprocessed)\n",
        "        tokens = preprocess_text(text)  # Assuming you have such a function\n",
        "        tokens_tensor = torch.tensor(tokens).to(device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(tokens_tensor)\n",
        "            prob = torch.sigmoid(output).cpu().numpy()[0]  # Get the probability\n",
        "\n",
        "        probabilities[label] = prob\n",
        "\n",
        "    # Create a DataFrame with the probabilities\n",
        "    df = pd.DataFrame(probabilities, index=[0])\n",
        "\n",
        "    # Normalize the probabilities using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_probabilities = scaler.fit_transform(df)\n",
        "\n",
        "    # Add unnormalized and normalized columns to the DataFrame\n",
        "    df_normalized = pd.DataFrame(normalized_probabilities, columns=df.columns)\n",
        "    df_normalized[\"normalized\"] = df_normalized.values.tolist()\n",
        "\n",
        "    df[\"unnormalized\"] = df.values.tolist()\n",
        "\n",
        "    return df, df_normalized\n",
        "\n",
        "# Example usage\n",
        "text = \"I'm really happy today\"\n",
        "df, df_normalized = get_emotion_probabilities(text, emotion_models)\n",
        "\n",
        "# Displaying unnormalized probabilities\n",
        "print(\"Probabilities (Unnormalized):\")\n",
        "print(df)\n",
        "\n",
        "# Displaying normalized probabilities\n",
        "print(\"Probabilities (Normalized):\")\n",
        "print(df_normalized)\n"
      ],
      "metadata": {
        "id": "saiVUGO1ImgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#white llm"
      ],
      "metadata": {
        "id": "_p4ZpxsWKo2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ef get_emotion_probabilities(text, emotion_models):\n",
        "    emotion_probs = []\n",
        "    for model in emotion_models:\n",
        "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "        emotion_prob = model(inputs['input_ids'], inputs['attention_mask']).squeeze().item()\n",
        "        emotion_probs.append(emotion_prob)\n",
        "    return torch.tensor(emotion_probs).unsqueeze(0).to(device)  # [1, num_emotions]\n",
        "\n",
        "# Instantiate models\n",
        "llm_model_name = \"bert-base-uncased\"  # Example LLM (you can choose a different one)\n",
        "emotion_models = [EmotionModel() for _ in range(5)]  # Replace with real emotion models\n",
        "\n",
        "# Create the Emotion-AUG LLM model\n",
        "emotion_augmented_llm = EmotionAugmentedLLM(llm_model_name, emotion_models).to(device)\n",
        "\n",
        "# Example text and emotion probabilities (for demo)\n",
        "texts = [\"I am so happy today!\", \"I feel so sad...\"]\n",
        "emotion_probs = [\n",
        "    [0.9, 0.05, 0.1, 0.2, 0.05],  # Example emotion probabilities for the first text\n",
        "    [0.1, 0.9, 0.2, 0.5, 0.05]   # Example emotion probabilities for the second text\n",
        "]\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = EmotionTextDataset(texts, emotion_probs, tokenizer=AutoTokenizer.from_pretrained(llm_model_name))\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "optimizer = optim.Adam(emotion_augmented_llm.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    emotion_augmented_llm.train()\n",
        "    running_loss = 0.0\n",
        "    for text, emotion_labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get emotion probabilities\n",
        "        emotion_features = get_emotion_probabilities(text, emotion_models)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = emotion_augmented_llm(text, emotion_features)\n",
        "\n",
        "        # Calculate loss and backpropagate\n",
        "        loss = criterion(outputs, emotion_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {running_loss/len(dataloader)}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(emotion_augmented_llm.state_dict(), \"emotion_augmented_llm.pth\")"
      ],
      "metadata": {
        "id": "f5xEUYCyKqNx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}