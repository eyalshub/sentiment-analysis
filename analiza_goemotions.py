# -*- coding: utf-8 -*-
"""analiza GoEmotions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uP0sPr3eH-RYLLtjVSZwa22WLRekr7eZ
"""

import nltk
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
# Download stop words list and punkt tokenizer (if not previously downloaded)
nltk.download('stopwords')
nltk.download('punkt')
!pip install gensim
!pip install pyLDAvis
!pip install wordcloud matplotlib

"""#loding data"""

!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv
!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv
!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv

goemotions_1 = pd.read_csv('data/full_dataset/goemotions_1.csv')
goemotions_2 = pd.read_csv('data/full_dataset/goemotions_2.csv')
goemotions_3 = pd.read_csv('data/full_dataset/goemotions_3.csv')

# איחוד הקבצים
combined_df = pd.concat([goemotions_1, goemotions_2, goemotions_3], ignore_index=True)
emotion_columns = combined_df.columns[9:]
# שמירה לקובץ חדש
combined_df['created_utc'] = pd.to_datetime(combined_df['created_utc'], unit='s', errors='coerce')
combined_df.to_csv('data/full_dataset/goemotions_combined.csv', index=False)

"""###text: The text of the comment (with masked tokens, as described in the paper).
###id: The unique id of the comment.
###author: The Reddit username of the comment's author.
###subreddit: The subreddit that the comment belongs to.
###link_id: The link id of the comment.
###parent_id: The parent id of the comment.
###created_utc: The timestamp of the comment.
###rater_id: The unique id of the annotator.
###example_very_unclear: Whether the annotator marked the example as being very ###unclear or difficult to label (in this case they did not choose any emotion labels).
###separate columns representing each of the emotion categories, with binary labels (0 or 1)

#General analysis
"""

missing_values = combined_df.isnull().sum()
print("Missing values in each column:")
print(missing_values)

unique_counts = combined_df.nunique()
print("Unique counts for each column:")
print(unique_counts)

emotion_counts = combined_df[emotion_columns].sum()
emotion_counts

overall_emotions_avg = combined_df[emotion_columns].mean()
overall_emotions_avg.sum()

"""#column analysis

##id
"""

rater_counts = combined_df.groupby('id')[emotion_columns].agg(lambda x: x.sum() > 0).sum(axis=1)

def more_emotom(num):
  more_than_two_raters = rater_counts[rater_counts > num]

  tagged_emotions = combined_df.groupby('id')[emotion_columns].sum()

  print(f"Comments rated by more than two raters and their tagged emotions: {len(more_than_two_raters)}")
  for comment_id in more_than_two_raters.index:
      emotions_tagged = tagged_emotions.loc[comment_id]

      tagged = emotions_tagged[emotions_tagged > 0].index.tolist()

      print(f"emotin more>{num} Comment ID: {comment_id}, Number of Raters: {more_than_two_raters[comment_id]}, Tagged Emotions: {tagged}")
more_emotom(10)

rater_counts = combined_df.groupby('id')[emotion_columns].agg(lambda x: x.sum() > 0).sum(axis=1)

more_than_two_raters = rater_counts[rater_counts > 2]

tagged_emotions = combined_df.groupby('id')[emotion_columns].sum()

results = []

for comment_id in more_than_two_raters.index:
    emotions_tagged = tagged_emotions.loc[comment_id]

    tagged = emotions_tagged[emotions_tagged > 0].index.tolist()

    comment_text = combined_df.loc[combined_df['id'] == comment_id, 'text'].values[0]

    results.append({
        'Comment ID': comment_id,
        'Number of Raters': more_than_two_raters[comment_id],
        'Tagged Emotions': tagged,
        'Comment Text': comment_text
    })

results_df = pd.DataFrame(results)

print("Comments rated by more than two raters and their tagged emotions:")
print(results_df)

"""##created_utc"""

(combined_df['created_utc'].min(),combined_df['created_utc'].max())

import pandas as pd
import matplotlib.pyplot as plt

# Convert 'created_utc' column to datetime format (if not already in datetime format)
combined_df['created_utc'] = pd.to_datetime(combined_df['created_utc'], unit='s')

# Extract day of the week (0 = Sunday, 6 = Saturday)
combined_df['day_of_week'] = combined_df['created_utc'].dt.dayofweek

# Extract hour of the day (0-23)
combined_df['hour_of_day'] = combined_df['created_utc'].dt.hour

# Calculate the frequency of comments for each day of the week
day_of_week_frequency = combined_df.groupby('day_of_week')['author'].count()

# Calculate the frequency of comments for each hour of the day
hour_of_day_frequency = combined_df.groupby('hour_of_day')['author'].count()

combined_df['day_of_month'] = combined_df['created_utc'].dt.day

# Calculate the frequency of comments for each day of the month
day_of_month_frequency = combined_df.groupby('day_of_month')['author'].count()
# Display the results
print("Frequency of comments by day of the week:")
print(day_of_week_frequency)

print("\nFrequency of comments by hour of the day:")
print(hour_of_day_frequency)

# Display the results
print("\nFrequency of comments by day of the month:")
print(day_of_month_frequency)

# Plot frequency of comments by day of the week
plt.figure(figsize=(10, 6))
day_of_week_frequency.plot(kind='bar')
plt.title("Frequency of Comments by Day of the Week")
plt.xlabel("Day of the Week")
plt.ylabel("Number of Comments")
plt.xticks(rotation=0)
plt.show()

# Plot frequency of comments by hour of the day
plt.figure(figsize=(10, 6))
hour_of_day_frequency.plot(kind='bar')
plt.title("Frequency of Comments by Hour of the Day")
plt.xlabel("Hour of the Day")
plt.ylabel("Number of Comments")
plt.xticks(rotation=0)
plt.show()


plt.figure(figsize=(10, 6))
day_of_month_frequency.plot(kind='bar')
plt.title("Frequency of Comments by Day of the Month")
plt.xlabel("Day of the Month")
plt.ylabel("Number of Comments")
plt.xticks(rotation=0)
plt.show()

total_avg = user_emotions.sum().sum() / user_emotions.size
total_avg

"""##author"""

author_counts = combined_df['author'].value_counts()
print("Author counts:\n", author_counts)

user_emotions = combined_df.groupby('author')[emotion_columns].sum()
def get_user_emotions(user):
    if user in user_emotions.index:
        user_1 = user_emotions.loc[user]
        exceeding_emotions = (user_1 > overall_emotions_avg).sum()
        print(f"All emotions for {user}: {user_1.sum()}")
        print(f"Number of emotions exceeding the average: {exceeding_emotions}")
        return user_1
    else:
        return f"User {user} not found in the data."

# Calling the function for the user '[deleted]'
print(get_user_emotions('[deleted]'))

def Use_emotional_abuse(num):
  user_comparison = user_emotions > overall_emotions_avg
  user_comparison_count = user_comparison.sum(axis=1)
  users_above = user_comparison_count[user_comparison_count >= num]
  return users_above

count_per_num = []

# Loop through each number from 1 to 27 and get the count of users who exceed that many emotions
for i in range(1, 28):
    count_per_num.append(len(Use_emotional_abuse(i)))

print(count_per_num)
# Plotting the results
plt.figure(figsize=(12, 6))
plt.bar(range(1, 28), count_per_num)
plt.title("Number of Users Exceeding Emotional Average by Number of Emotions")
plt.xlabel("Number of Exceeded Emotions")
plt.ylabel("Number of Users")
plt.xticks(range(1, 28))
plt.show()

def count_raters_above_emotion_threshold(emotion, percent):
    if emotion not in user_emotions.columns:
        return f"Emotion '{emotion}' not found in the data."

    total_tags_per_rater = user_emotions.sum(axis=1)

    emotion_percent_per_rater = (user_emotions[emotion] / total_tags_per_rater) * 100

    raters_above_threshold = emotion_percent_per_rater[emotion_percent_per_rater >= percent]

    return list(raters_above_threshold.index)
emotion = 'anger'
percent = 100
print(f"Number of raters tagging '{emotion}' above {percent}%: {len(count_raters_above_emotion_threshold(emotion, percent))} {count_raters_above_emotion_threshold(emotion, percent)}  ")

"""## rater_id"""

emotion_by_rater = combined_df.groupby('rater_id')[emotion_columns].sum()

rater_dfs = {}

for rater_id, rater_df in combined_df.groupby('rater_id'):
    rater_dfs[rater_id] = rater_df

for rater_id, rater_df in rater_dfs.items():
    num_records = len(rater_df)
    print(f"Data for rater ID '{rater_id}' with {num_records} records:")


    emotion_counts = {emotion: (rater_df[emotion] == 1).sum() for emotion in emotion_columns}

    for emotion, count in emotion_counts.items():
        print(f"Number of occurrences of emotion '{emotion}': {count}")

    emotion_percentages = {}
    for emotion in emotion_columns:
        if num_records > 0:
            emotion_percentages[emotion] = (emotion_counts[emotion] / num_records) * 100
        else:
            emotion_percentages[emotion] = 0

    for emotion, percentage in emotion_percentages.items():
        print(f"Percentage of '{emotion}': {percentage:.2f}%")

    print("\n")

import pandas as pd
import matplotlib.pyplot as plt

rater_dfs = {}

for rater_id, rater_df in combined_df.groupby('rater_id'):
    rater_dfs[rater_id] = rater_df

rater_analysis = []

for rater_id, rater_df in rater_dfs.items():
    num_records = len(rater_df)

    emotion_counts = {emotion: (rater_df[emotion] == 1).sum() for emotion in emotion_columns}

    emotion_percentages = {}
    for emotion in emotion_columns:
        emotion_percentages[emotion] = (emotion_counts[emotion] / num_records) * 100 if num_records > 0 else 0

    rater_analysis.append({
        'rater_id': rater_id,
        'num_records': num_records,
        **emotion_percentages
    })

analysis_df = pd.DataFrame(rater_analysis)

print(analysis_df)

analysis_df.set_index('rater_id')[emotion_columns].T.plot(kind='bar', figsize=(26, 14))

plt.title('Emotion Percentages by Rater ID')
plt.xlabel('Emotions')
plt.ylabel('Percentage')
plt.xticks(rotation=45)

plt.legend(title='Rater ID', loc='upper left', bbox_to_anchor=(1, 1))

plt.show()

def get_rater_emotion_distribution(rater_id):
    if rater_id not in emotion_by_rater.index:
        return f"Rater ID {rater_id} not found in the data."

    rater_emotions = emotion_by_rater.loc[rater_id]

    total_tags = rater_emotions.sum()

    emotion_distribution = rater_emotions / total_tags

    plt.figure(figsize=(10, 6))
    emotion_distribution.plot(kind='bar', color='skyblue')
    plt.title(f"Emotion Distribution for Rater ID: {rater_id}")
    plt.xlabel("Emotion")
    plt.ylabel("Proportion of Tags")
    plt.xticks(rotation=45)
    plt.show()

    return emotion_distribution, total_tags

rater_id = 'example_rater_id'
emotion_distribution, total_tags = get_rater_emotion_distribution(21)
print(f"Emotion Distribution for {rater_id}:\n{emotion_distribution}")
print(f"Total Tags by {rater_id}: {total_tags}")

def count_raters_above_emotion_threshold(emotion, percent):
    if emotion not in emotion_by_rater.columns:
        return f"Emotion '{emotion}' not found in the data."

    total_tags_per_rater = emotion_by_rater.sum(axis=1)

    emotion_percent_per_rater = (emotion_by_rater[emotion] / total_tags_per_rater) * 100

    raters_above_threshold = emotion_percent_per_rater[emotion_percent_per_rater >= percent]

    return list(raters_above_threshold.index)
emotion = 'neutral'
percent = 10
print(f"Number of raters tagging '{emotion}' above {percent}%: {count_raters_above_emotion_threshold(emotion, percent)}")

"""## link_id"""

link_comment_count = combined_df['link_id'].value_counts()

print("Top 10 link_ids by comment count:")
print(link_comment_count.head(10))

plt.figure(figsize=(12, 6))
link_comment_count.head(10).plot(kind='bar')
plt.title("Top 10 link_ids by Number of Comments")
plt.xlabel("link_id")
plt.ylabel("Number of Comments")
plt.show()

"""##parent_id"""

parent_comment_count = combined_df['parent_id'].value_counts()

print("Top 10 parent_ids by reply count:")
print(parent_comment_count.head(10))

plt.figure(figsize=(12, 6))
parent_comment_count.head(10).plot(kind='bar')
plt.title("Top 10 parent_ids by Number of Replies")
plt.xlabel("parent_id")
plt.ylabel("Number of Replies")
plt.show()

link_parent_count = combined_df.groupby(['link_id', 'parent_id']).size().reset_index(name='reply_count')

print("Top 10 link_id and parent_id combinations by reply count:")
print(link_parent_count.sort_values(by='reply_count', ascending=False).head(10))

linked_comments_df = combined_df[combined_df['parent_id'].isin(combined_df['link_id'].unique())]

emotion_sums = linked_comments_df[emotion_columns].sum()

print("Sum of emotions in linked comments:")
print(len(linked_comments_df))
print(emotion_sums)

plt.figure(figsize=(12, 6))
emotion_sums.plot(kind='bar', color='skyblue')
plt.title("Distribution of Emotions in Linked Comments")
plt.xlabel("Emotion Category")
plt.ylabel("Sum of Emotion Counts")
plt.xticks(rotation=45)
plt.show()

linked_comments_df['hour_of_day'] = linked_comments_df['created_utc'].dt.hour

hourly_comment_count = linked_comments_df['hour_of_day'].value_counts().sort_index()

plt.figure(figsize=(12, 6))
hourly_comment_count.plot(kind='bar', color='coral')
plt.title("Number of Linked Comments by Hour of the Day")
plt.xlabel("Hour of Day")
plt.ylabel("Number of Linked Comments")
plt.show()

linked_comments_df['day_of_week'] = linked_comments_df['created_utc'].dt.dayofweek

daily_comment_count = linked_comments_df['day_of_week'].value_counts().sort_index()

plt.figure(figsize=(12, 6))
daily_comment_count.plot(kind='bar', color='green')
plt.title("Number of Linked Comments by Day of the Week")
plt.xlabel("Day of the Week (0=Sunday, 6=Saturday)")
plt.ylabel("Number of Linked Comments")
plt.xticks(rotation=0)
plt.show()

import networkx as nx
import matplotlib.pyplot as plt

G = nx.DiGraph()

for _, row in linked_comments_df.iterrows():
    parent_id = row['parent_id']
    comment_id = row['id']

    G.add_edge(parent_id, comment_id)

plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G, k=0.1)
nx.draw(G, pos, with_labels=True, node_size=20, node_color="skyblue", edge_color="gray", font_size=8, alpha=0.7)
plt.title("Comment Network Graph")
plt.show()

"""##subreddit"""

subreddit_dfs = {}

for subreddit, group in combined_df.groupby('subreddit'):
    subreddit_dfs[subreddit] = group.reset_index(drop=True)  # מאחסן את ה-DataFrame של כל subreddit

for subreddit, subreddit_df in subreddit_dfs.items():
    num_records = len(subreddit_df)
    print(f"Data for subreddit '{subreddit}': with {num_records} records:")

    emotion_counts = {emotion: (subreddit_df[emotion] == 1).sum() for emotion in emotion_columns}

    num_records = len(subreddit_df)

    for emotion, count in emotion_counts.items():
        print(f"Number of occurrences of emotion '{emotion}': {count}")

    emotion_percentages = {}
    for emotion in emotion_columns:
        if num_records > 0:
            emotion_percentages[emotion] = (emotion_counts[emotion] / num_records) * 100
        else:
            emotion_percentages[emotion] = 0

    for emotion, percentage in emotion_percentages.items():
        print(f"Percentage of '{emotion}': {percentage:.2f}%")

    print("\n")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def analyze_subreddit_stats(subreddit_name, status_df, emotion_columns, date_column='timestamp'):
    subreddit_data = status_df[status_df['subreddit'] == subreddit_name]

    if subreddit_data.empty:
        print(f"No data found for subreddit: {subreddit_name}")
        return

    subreddit_data[date_column] = pd.to_datetime(subreddit_data[date_column])

    emotion_means = subreddit_data[emotion_columns].mean()
    print(f"Average emotions for subreddit {subreddit_name}:\n{emotion_means}\n")

    plt.figure(figsize=(8, 4))
    sns.heatmap(emotion_means.to_frame().T, cmap="coolwarm", annot=True, fmt=".2f", linewidths=0.5)
    plt.title(f"Average Emotions for Subreddit: {subreddit_name}")
    plt.xlabel("Emotion Type")
    plt.yticks([])
    plt.tight_layout()
    plt.show()

    subreddit_data['date'] = subreddit_data[date_column].dt.date
    comments_by_date = subreddit_data['date'].value_counts().sort_index()

    plt.figure(figsize=(12, 6))
    comments_by_date.plot(kind='bar', color='skyblue', edgecolor='black')
    plt.title(f"Number of Comments by Date for Subreddit: {subreddit_name}")
    plt.xlabel("Date")
    plt.ylabel("Number of Comments")
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()


subreddit_name = 'politics'
analyze_subreddit_stats(subreddit_name, combined_df, emotion_columns, date_column='created_utc')

import matplotlib.pyplot as plt
import networkx as nx
import seaborn as sns
import pandas as pd

def build_network_graph_for_subreddit(subreddit_name, combined_df, emotion_columns):
    # Filter the dataframe by the specified subreddit
    subreddit_data = combined_df[combined_df['subreddit'] == subreddit_name]

    if subreddit_data.empty:
        print(f"No data found for subreddit: {subreddit_name}")
        return

    # Create a directed graph using networkx
    G = nx.DiGraph()

    # Add nodes for each comment
    for index, comment in subreddit_data.iterrows():
        comment_id = comment['id']
        author = comment['author']
        text = comment['text']

        # Get emotion data for the comment
        comment_emotions = comment[emotion_columns].mean()  # Calculate the mean emotions for the comment

        # Add node with emotion data
        G.add_node(comment_id, author=author, text=text, emotions=comment_emotions)

        # If the comment has a parent (is a reply), add an edge to the parent
        parent_id = comment['parent_id']
        if pd.notnull(parent_id) and parent_id != comment_id:
            G.add_edge(parent_id, comment_id)

    # Plotting the graph
    plt.figure(figsize=(20, 14))
    pos = nx.spring_layout(G, seed=42)  # Layout for positioning nodes

    # Draw nodes and edges
    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')
    nx.draw_networkx_edges(G, pos, alpha=0.5)
    nx.draw_networkx_labels(G, pos, font_size=8, font_color='black')

    # Display the graph title
    plt.title(f"Network Graph for Subreddit: {subreddit_name} with Emotions")
    plt.show()
def analyze_subreddit_emotions(subreddit_name, combined_df, emotion_columns):
    # Filter the dataframe by the specified subreddit
    subreddit_data = combined_df[combined_df['subreddit'] == subreddit_name]

    if subreddit_data.empty:
        print(f"No data found for subreddit: {subreddit_name}")
        return

    # Calculate the mean emotion values for each emotion type within the subreddit
    subreddit_emotions = subreddit_data[emotion_columns].mean()

    # Reshape the data for heatmap plotting
    emotion_df = subreddit_emotions.to_frame().T  # Convert to DataFrame and transpose for proper plotting

    # Plot the emotion heatmap
    plt.figure(figsize=(12, 7))
    sns.heatmap(emotion_df, cmap='coolwarm', annot=True, fmt=".2f", cbar=True, linewidths=0.5)
    plt.title(f"Average Emotion Heatmap for Subreddit: {subreddit_name}", fontsize=16)
    plt.xlabel("Emotion Type", fontsize=12)
    plt.ylabel("Average Emotion Score", fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()


# Example usage:
subreddit_name = 'China'  # Replace with the subreddit you want to analyze

# Build and display the network graph with emotions for the subreddit
build_network_graph_for_subreddit(subreddit_name, combined_df, emotion_columns)

# Analyze and display the emotions for the subreddit
analyze_subreddit_emotions(subreddit_name, combined_df, emotion_columns)

import pandas as pd

def build_comment_status_dict(combined_df):
    # Create an empty dictionary to store the statuses
    status_dict = {}

    # Loop over all rows in the dataframe
    for idx, row in combined_df.iterrows():
        comment_id = row['id']
        parent_id = row['parent_id']
        link_id = row['link_id']
        comment_author = row['author']
        comment_text = row['text']
        subreddit = row['subreddit']  # Assuming 'subreddit' column exists in your dataframe

        # If parent_id is NaN, use link_id as the status key
        status_key = parent_id if pd.notnull(parent_id) else link_id

        # Initialize the key in the dictionary if it doesn't exist yet
        if status_key not in status_dict:
            status_dict[status_key] = {
                'comments': [],
                'subreddits': set()  # To store unique subreddits for each status
            }

        # Append the comment to the list of comments for that parent_id or link_id
        status_dict[status_key]['comments'].append({
            'comment_id': comment_id,
            'author': comment_author,
            'text': comment_text
        })

        # Add the subreddit to the set for that status
        status_dict[status_key]['subreddits'].add(subreddit)

    return status_dict

def count_comments_by_status(status_dict):
    # Count the number of comments for each status and get the associated subreddits
    comment_counts = {}
    for status_key, data in status_dict.items():
        comment_counts[status_key] = {
            'count': len(data['comments']),
            'subreddits': list(data['subreddits'])  # List of unique subreddits for each status
        }

    return comment_counts

# Example usage:
# Assuming `combined_df` is your dataframe
status_dict = build_comment_status_dict(combined_df)

# Count the comments for each status along with subreddits
comment_counts = count_comments_by_status(status_dict)

# Print out the number of comments and subreddits for each status
for status_key, data in comment_counts.items():
    subreddits_str = ', '.join(data['subreddits']) if data['subreddits'] else 'No subreddits'
    print(f"Status {status_key} has {data['count']} comments. Subreddits: {subreddits_str}")

status_dict

import matplotlib.pyplot as plt
import networkx as nx
import seaborn as sns

def build_network_graph_for_status(status_key, status_dict, emotion_columns):
    # Create a directed graph using networkx
    G = nx.DiGraph()

    # Get the data for the status
    comments_data = status_dict.get(status_key, {}).get('comments', [])

    if not comments_data:
        print(f"No comments found for status {status_key}")
        return

    # Add nodes to the graph (one for each comment)
    for comment in comments_data:
        comment_id = comment['comment_id']
        author = comment['author']
        text = comment['text']

        # Add node with emotion data
        # Assuming you have emotion columns in the DataFrame
        comment_emotions = combined_df[combined_df['id'] == comment_id][emotion_columns].mean().to_dict()

        # Add node with emotion data
        G.add_node(comment_id, author=author, text=text, emotions=comment_emotions)

        # If the comment has a parent (is a reply), add an edge to the parent
        parent_id = combined_df[combined_df['id'] == comment_id]['parent_id'].values[0]
        if pd.notnull(parent_id):
            G.add_edge(parent_id, comment_id)

    # Plotting the graph
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G, seed=42)  # Layout for positioning nodes

    # Draw nodes and edges
    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')
    nx.draw_networkx_edges(G, pos, alpha=0.5)
    nx.draw_networkx_labels(G, pos, font_size=8, font_color='black')

    # Adding edge labels for emotions (optional)
    edge_labels = nx.get_edge_attributes(G, 'emotion')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

    plt.title(f"Network Graph for Status {status_key} with Emotions")
    plt.show()

def analyze_status_emotions(status_key, status_dict, emotion_columns):
    # Retrieve the comments associated with the status
    comments_data = status_dict.get(status_key, {}).get('comments', [])

    if not comments_data:
        print(f"No comments found for status {status_key}")
        return

    # For each comment, analyze the emotions
    emotion_data = []
    for comment in comments_data:
        comment_id = comment['comment_id']
        comment_emotions = combined_df[combined_df['id'] == comment_id][emotion_columns].mean().to_dict()

        # Add emotions data for this comment
        emotion_data.append(comment_emotions)

    # Convert the emotion data into a DataFrame for easy analysis
    emotion_df = pd.DataFrame(emotion_data)

    # Plot a heatmap of emotions
    plt.figure(figsize=(12, 6))
    sns.heatmap(emotion_df.T, cmap='YlGnBu', annot=True, fmt=".2f", cbar=True)
    plt.title(f"Emotion Heatmap for Status {status_key}")
    plt.xlabel("Comment ID")
    plt.ylabel("Emotion Type")
    plt.show()

# Example usage:
# Assuming `status_dict` is the dictionary of statuses built earlier
status_key = 't1_efam9kw'  # Replace with the status you want to analyze

# Build and display the network graph with emotions
build_network_graph_for_status(status_key, status_dict, emotion_columns)

# Analyze and display the emotions for this status
analyze_status_emotions(status_key, status_dict, emotion_columns)

import pandas as pd

def convert_status_dict_to_dataframe(status_dict, emotion_columns, combined_df):
    # Lists to hold the data for each column in the DataFrame
    comments_list = []
    subreddits_list = []
    emotions_list = []

    # Iterate through the status dictionary
    for status_key, data in status_dict.items():
        for comment_info in data['comments']:
            comment_id = comment_info['comment_id']

            # Fetch comment data for each comment
            comment_data = combined_df[combined_df['id'] == comment_id]
            if comment_data.empty:
                continue  # Skip if no data found for this comment

            comment = comment_data.iloc[0]
            comment_emotions = comment[emotion_columns].mean()  # Calculate the mean emotions for the comment

            # Append data to the lists
            comments_list.append(comment)
            subreddits_list.append(list(data['subreddits']))  # List of subreddits for each status
            emotions_list.append(comment_emotions)

    # Create a DataFrame from the collected data
    status_df = pd.DataFrame(comments_list)

    # Add subreddit and emotions columns
    status_df['subreddits'] = subreddits_list
    status_df['emotions'] = emotions_list

    return status_df

# Example usage:
status_dict = build_comment_status_dict(combined_df)

# Convert status_dict to a DataFrame
status_df = convert_status_dict_to_dataframe(status_dict, emotion_columns, combined_df)

# Display the DataFrame
print(status_df.head())