{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRLOyoaMAEFU"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard\n",
        "!pip install gensim\n",
        "!pip install pyLDAvis\n",
        "!pip install wordcloud matplotlib\n",
        "!pip install emoji\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "!gzip -d cc.en.300.bin.gz\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict, Counter\n",
        "import pandas as pd\n",
        "import emoji\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, precision_recall_curve, auc\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vl_JMw_qAPzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
      ],
      "metadata": {
        "id": "JZb4ySItASYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93ff4b1-af8b-4192-f882-c80fb82cdab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-19 13:01:12--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 142.251.175.207, 74.125.24.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14174600 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_1.csv.1’\n",
            "\n",
            "goemotions_1.csv.1  100%[===================>]  13.52M  7.63MB/s    in 1.8s    \n",
            "\n",
            "2025-03-19 13:01:14 (7.63 MB/s) - ‘data/full_dataset/goemotions_1.csv.1’ saved [14174600/14174600]\n",
            "\n",
            "--2025-03-19 13:01:14--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 142.251.175.207, 74.125.24.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14173154 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_2.csv.1’\n",
            "\n",
            "goemotions_2.csv.1  100%[===================>]  13.52M  7.67MB/s    in 1.8s    \n",
            "\n",
            "2025-03-19 13:01:16 (7.67 MB/s) - ‘data/full_dataset/goemotions_2.csv.1’ saved [14173154/14173154]\n",
            "\n",
            "--2025-03-19 13:01:16--  https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 142.251.175.207, 74.125.24.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14395164 (14M) [application/octet-stream]\n",
            "Saving to: ‘data/full_dataset/goemotions_3.csv.1’\n",
            "\n",
            "goemotions_3.csv.1  100%[===================>]  13.73M  7.60MB/s    in 1.8s    \n",
            "\n",
            "2025-03-19 13:01:18 (7.60 MB/s) - ‘data/full_dataset/goemotions_3.csv.1’ saved [14395164/14395164]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "goemotions_1 = pd.read_csv('data/full_dataset/goemotions_1.csv')\n",
        "goemotions_2 = pd.read_csv('data/full_dataset/goemotions_2.csv')\n",
        "goemotions_3 = pd.read_csv('data/full_dataset/goemotions_3.csv')\n",
        "\n",
        "combined_df = pd.concat([goemotions_1, goemotions_2, goemotions_3], ignore_index=True)\n",
        "emotion_columns = combined_df.columns[9:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "combined_df = pd.read_csv('/content/emotional_data.csv')\n",
        "\n",
        "combined_df['created_utc'] = pd.to_datetime(combined_df['created_utc'], unit='s', errors='coerce')\n",
        "combined_df.to_csv('data/full_dataset/goemotions_combined.csv', index=False)\n",
        "combined_df = combined_df.query(\"example_very_unclear != True\")"
      ],
      "metadata": {
        "id": "4dYrzKlyAT6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#work white emoji..."
      ],
      "metadata": {
        "id": "wFGv5d80AWiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dictionary to store emojis with their positions and texts\n",
        "emoji_dict = {}\n",
        "\n",
        "# Function to extract emojis and build the dictionary\n",
        "def build_emoji_dict(df):\n",
        "    for idx, row in df.iterrows():\n",
        "        text = row['text']\n",
        "        for char in text:\n",
        "            if emoji.is_emoji(char):\n",
        "                if char not in emoji_dict:\n",
        "                    emoji_dict[char] = []\n",
        "                emoji_dict[char].append((idx, text))\n",
        "build_emoji_dict(combined_df)\n"
      ],
      "metadata": {
        "id": "U5yorT0uAZ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process"
      ],
      "metadata": {
        "id": "7sT_g1m9AiMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"[<>]\", \"\", text)\n",
        "    text = f\"[START] {text} [END]\"\n",
        "\n",
        "    text = re.sub(r'\\[(\\w+)\\]', r'<\\1>', text)\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9<> ]\", \"\", text)\n",
        "\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "combined_df['processed_text'] = combined_df['text'].apply(preprocess_text)\n",
        "\n",
        "print(combined_df[['text', 'processed_text']].head())"
      ],
      "metadata": {
        "id": "Sjt1V2V2Akgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a808b7-a176-44ef-ac1b-6187f8b2860b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0     You do right, if you don't care then fuck 'em!   \n",
            "1  Right? Considering it’s such an important docu...   \n",
            "2  He isn't as big, but he's still quite popular....   \n",
            "3                                that's adorable asf   \n",
            "4  \"Sponge Blurb Pubs Quaw Haha GURR ha AAa!\" fin...   \n",
            "\n",
            "                                      processed_text  \n",
            "0  <start> you do right if you dont care then fuc...  \n",
            "1  <start> right considering its such an importan...  \n",
            "2  <start> he isnt as big but hes still quite pop...  \n",
            "3                   <start> thats adorable asf <end>  \n",
            "4  <start> sponge blurb pubs quaw haha gurr ha aa...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mappings(df):\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "    index_counter = 1\n",
        "\n",
        "    for text in df['processed_text']:\n",
        "        words = text.split()\n",
        "\n",
        "        for word in words:\n",
        "            if word not in word_to_index:\n",
        "                word_to_index[word] = index_counter\n",
        "                index_to_word[index_counter] = word\n",
        "                index_counter += 1\n",
        "\n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "\n",
        "word_to_index, index_to_word = create_mappings(combined_df)\n",
        "\n",
        "print(\"Word to Index Mapping:\", dict(list(word_to_index.items())[:10]))\n",
        "print(\"Index to Word Mapping:\", dict(list(index_to_word.items())[:10]))"
      ],
      "metadata": {
        "id": "yt17tYfnAo3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d970d91f-8964-4f4d-b43b-cae0f65af639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word to Index Mapping: {'<start>': 1, 'you': 2, 'do': 3, 'right': 4, 'if': 5, 'dont': 6, 'care': 7, 'then': 8, 'fuck': 9, 'em': 10}\n",
            "Index to Word Mapping: {1: '<start>', 2: 'you', 3: 'do', 4: 'right', 5: 'if', 6: 'dont', 7: 'care', 8: 'then', 9: 'fuck', 10: 'em'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_to_index),len(index_to_word)"
      ],
      "metadata": {
        "id": "zWcMuJzbAtoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb9e282-002e-42a9-ba9e-fba6af89213a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24940, 24940)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_index_sequence(text, word_to_index):\n",
        "    words = text.split()\n",
        "    index_sequence = [word_to_index[word] for word in words if word in word_to_index]\n",
        "    return index_sequence\n",
        "combined_df['text_as_indexes'] = combined_df['processed_text'].apply(lambda x: text_to_index_sequence(x, word_to_index))\n",
        "print(combined_df[['text', 'processed_text', 'text_as_indexes']].head())"
      ],
      "metadata": {
        "id": "68zDtA01AvJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b6d78a-0c31-4511-d865-529541628563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0     You do right, if you don't care then fuck 'em!   \n",
            "1  Right? Considering it’s such an important docu...   \n",
            "2  He isn't as big, but he's still quite popular....   \n",
            "3                                that's adorable asf   \n",
            "4  \"Sponge Blurb Pubs Quaw Haha GURR ha AAa!\" fin...   \n",
            "\n",
            "                                      processed_text  \\\n",
            "0  <start> you do right if you dont care then fuc...   \n",
            "1  <start> right considering its such an importan...   \n",
            "2  <start> he isnt as big but hes still quite pop...   \n",
            "3                   <start> thats adorable asf <end>   \n",
            "4  <start> sponge blurb pubs quaw haha gurr ha aa...   \n",
            "\n",
            "                                     text_as_indexes  \n",
            "0             [1, 2, 3, 4, 5, 2, 6, 7, 8, 9, 10, 11]  \n",
            "1  [1, 4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,...  \n",
            "2  [1, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41...  \n",
            "3                                [1, 50, 51, 52, 11]  \n",
            "4  [1, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#word2Vec"
      ],
      "metadata": {
        "id": "YysTC3V9AzZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_word2vec(df):\n",
        "    processed_texts = df['processed_text'].apply(lambda x: x.split())\n",
        "    return processed_texts\n",
        "\n",
        "sentences = prepare_data_for_word2vec(combined_df)\n",
        "model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
        "word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
        "print(\"Word vector for 'start':\", word_vectors['start'])"
      ],
      "metadata": {
        "id": "95Kj5GgwA1WS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64f6f6d-b058-475d-c987-aadac9514504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'start': [ 1.43997356e-01  9.80936646e-01 -3.22280437e-01  2.49828976e-02\n",
            " -8.49179849e-02 -2.26509571e-01  3.45500469e-01  8.84381831e-01\n",
            "  1.90539196e-01 -1.47458106e-01  3.29569787e-01 -2.74357289e-01\n",
            "  8.49218853e-03 -1.27615020e-01 -9.95273367e-02 -3.96747649e-01\n",
            "  6.29019290e-02  1.94452032e-01  1.89422190e-01  1.04487449e-01\n",
            " -2.46406734e-01 -3.04845702e-02  1.73779115e-01  4.80746686e-01\n",
            " -1.16303109e-01  4.81422752e-01 -8.25350106e-01  3.59569751e-02\n",
            "  1.84139535e-01 -3.84169847e-01 -1.22533785e-02 -6.15075648e-01\n",
            " -2.55221367e-01 -8.56387690e-02 -4.78570610e-02 -9.59383324e-02\n",
            "  4.25008655e-01 -6.53701961e-01  2.58525908e-01 -9.08055827e-02\n",
            " -1.70327172e-01 -3.63490015e-01  4.46965426e-01 -4.13592875e-01\n",
            "  6.71687782e-01  3.68146479e-01  2.58491069e-01 -1.95627138e-01\n",
            " -1.15605950e-01  3.02469552e-01  1.38134032e-01 -1.73135892e-01\n",
            " -2.81258196e-01  2.55588908e-02 -1.56331405e-01  8.43905434e-02\n",
            "  9.14063752e-02  1.96273342e-01  1.28584847e-01  6.05602004e-02\n",
            " -2.58519292e-01 -6.85845315e-02 -2.85409391e-01  1.58434376e-01\n",
            "  1.71405151e-01  1.26565486e-01  9.33511406e-02  6.56871140e-01\n",
            "  1.63168013e-02 -1.75394043e-01 -3.62679780e-01  2.54960090e-01\n",
            "  3.34719598e-01 -3.12423199e-01 -1.91668764e-01  1.49298698e-01\n",
            " -3.21966499e-01  1.36811152e-01 -2.30147690e-01  7.04498827e-01\n",
            " -4.48325574e-01 -4.72461015e-01 -2.30780080e-01  7.97294974e-01\n",
            "  1.85291782e-01  1.34886369e-01 -6.66929603e-01 -2.98621152e-02\n",
            " -7.92311653e-02  2.26502910e-01  5.11870444e-01 -2.70085633e-01\n",
            "  4.07235958e-02  4.59220484e-02  2.05115780e-01  7.42036924e-02\n",
            "  1.08110569e-01 -2.77400285e-01 -4.72171605e-01  5.41473739e-02\n",
            "  3.35011259e-02 -9.21918452e-02 -1.66258827e-01  5.83254576e-01\n",
            "  3.59368712e-01 -6.49109483e-02 -2.14584887e-01  8.10473114e-02\n",
            " -3.62987965e-01  2.70267487e-01 -1.96684048e-01 -3.61558720e-02\n",
            " -4.10555676e-03  3.80942136e-01  2.68884063e-01  9.54589341e-03\n",
            "  4.68345322e-02  6.53392524e-02  3.65088850e-01 -7.94833750e-02\n",
            "  9.91863534e-02  2.27712065e-01  9.36677828e-02 -9.76065025e-02\n",
            "  6.57901987e-02 -2.51929760e-01  1.62250489e-01 -2.42492601e-01\n",
            " -1.77561685e-01  4.45976555e-01 -2.68025517e-01  2.51873553e-01\n",
            "  4.06519651e-01 -2.83050209e-01 -2.48021018e-02  3.90820235e-01\n",
            "  1.99675918e-01 -4.04870123e-01 -2.44352281e-01 -1.24909937e-01\n",
            "  2.28695184e-01 -1.79020345e-01 -5.80491647e-02  8.76846910e-02\n",
            "  2.62227535e-01 -8.78482535e-02 -5.84648311e-01  1.39235839e-01\n",
            "  3.46618891e-01 -2.92740464e-01 -4.07623015e-02 -1.57299355e-01\n",
            "  3.48770440e-01 -3.76539737e-01 -1.02155954e-01  2.94146866e-01\n",
            " -3.33754331e-01 -1.24698490e-01 -1.97846755e-01  1.36710346e-01\n",
            "  3.48818004e-01  1.53187841e-01 -4.01881456e-01  5.39265692e-01\n",
            " -3.10849786e-01  9.57672764e-03 -2.53893346e-01  2.11342618e-01\n",
            " -9.07991752e-02  3.06433707e-01 -3.74105483e-01 -6.54577510e-03\n",
            "  1.47146866e-01 -5.75679652e-02  8.76235291e-02 -1.26007304e-01\n",
            "  2.81607211e-01 -2.34415933e-01  6.06627725e-02 -2.77462214e-01\n",
            " -3.25170942e-02 -1.85394049e-01 -3.82710695e-01  7.61438832e-02\n",
            " -1.45197749e-01  2.10796967e-01  5.36450505e-01  1.71930715e-01\n",
            "  9.80550945e-02  3.58553708e-01  2.62797713e-01  6.91502243e-02\n",
            " -5.46608984e-01  2.21148297e-01  5.76471128e-02 -2.05822498e-01\n",
            " -2.65728712e-01 -1.65347725e-01 -8.18827823e-02 -1.63261760e-02\n",
            " -5.48353672e-01 -5.13445102e-02  5.32214761e-01 -1.24247096e-01\n",
            "  9.35181230e-02 -1.13746271e-01  1.43873587e-01 -2.25738689e-01\n",
            "  2.71946900e-02 -6.05790615e-02  2.61241466e-01 -1.94866821e-01\n",
            "  5.69472164e-02 -6.95392430e-01  2.63850421e-01 -4.52331081e-02\n",
            " -2.42453828e-01 -9.43433106e-01 -1.08615406e-01 -4.50059086e-01\n",
            "  1.46500438e-01  1.99041009e-01 -2.19814137e-01  1.82251409e-01\n",
            " -1.86861843e-01  1.22925982e-01 -6.56333491e-02 -2.26463348e-01\n",
            " -3.93506497e-01 -3.22131127e-01  2.89624691e-01  1.58900470e-01\n",
            "  1.61663696e-01  3.17482688e-02 -4.23159242e-01  1.69775009e-01\n",
            " -7.31984526e-02 -2.37031594e-01  3.39370817e-02 -5.68309128e-01\n",
            "  1.47773445e-01  6.36150986e-02 -8.53570702e-04 -1.92725156e-02\n",
            "  4.86099198e-02 -3.46338093e-01 -3.00488085e-01  2.33422115e-01\n",
            " -2.65540630e-01  1.44390419e-01  1.66661799e-01  1.10382296e-01\n",
            "  1.34041041e-01  1.57711163e-01 -4.56277311e-01 -2.13788785e-02\n",
            "  2.92422801e-01  4.31314111e-01 -7.73164570e-01 -2.12549657e-01\n",
            " -1.07992515e-01  5.39126158e-01  1.02215968e-01 -3.31465483e-01\n",
            " -1.19465947e-01 -1.13155633e-01  3.09347138e-02  1.00435242e-01\n",
            " -3.38098370e-02  3.13771695e-01 -2.58225024e-01  3.26645374e-01\n",
            " -1.00855440e-01 -3.70850474e-01  5.63609064e-01 -8.17507207e-02\n",
            "  2.07382113e-01  3.14970575e-02 -2.17975244e-01 -3.96888196e-01\n",
            "  1.37767494e-01  3.50708365e-01 -2.43382841e-01  3.70438337e-01\n",
            " -9.32945237e-02  1.66172162e-01 -3.52018446e-01  1.39062703e-01\n",
            "  6.17968887e-02  1.06014535e-02  2.85142332e-01  1.87188938e-01\n",
            "  5.68758771e-02  2.82425340e-02  1.41858816e-01  6.29530728e-01\n",
            "  2.30913579e-01 -4.33951497e-01  4.06788923e-02 -2.74285793e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FestText"
      ],
      "metadata": {
        "id": "BLKSIvWjA3dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "fasttext_model = fasttext.load_model('/content/cc.en.300.bin')\n",
        "\n",
        "def prepare_data_for_fasttext(df):\n",
        "    processed_texts = df['processed_text'].apply(lambda x: x.split())\n",
        "    return processed_texts\n",
        "\n",
        "sentences = prepare_data_for_fasttext(combined_df)\n",
        "\n",
        "word_vectors = {word: fasttext_model.get_word_vector(word) for sentence in sentences for word in sentence}\n",
        "\n",
        "print(\"Word vector for 'start':\", word_vectors.get('start', 'Word not found'))\n"
      ],
      "metadata": {
        "id": "FsmIXkupAyay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e36abc-5a92-4ac5-b3c0-b30d98bad63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'start': [ 7.91064836e-03 -9.43343155e-03  2.28888560e-02  1.15760624e-01\n",
            " -5.44784889e-02  3.52381244e-02  1.51470797e-02  9.68480669e-03\n",
            "  3.72007266e-02 -8.26112553e-03 -3.63847502e-02  7.02371597e-02\n",
            " -1.53566375e-02  1.11012265e-01  1.33964136e-01  6.41972050e-02\n",
            "  6.07942492e-02 -1.30296145e-02  1.28229149e-04  9.35186446e-02\n",
            " -6.10206239e-02 -2.02715285e-02 -1.50931003e-02 -9.10209268e-02\n",
            "  1.57276485e-02 -1.85630284e-02 -6.97859824e-02  6.44239336e-02\n",
            " -4.34500724e-02  3.08100097e-02  2.29679141e-03 -5.58396354e-02\n",
            " -6.58368096e-02 -6.34431988e-02  3.13772000e-02  2.18865033e-02\n",
            "  3.18698958e-02  1.05855837e-01 -2.08669528e-03 -9.21282396e-02\n",
            "  3.49404141e-02 -9.22180153e-03  8.44760537e-02  6.59895763e-02\n",
            " -3.71814519e-02  7.32254684e-02  3.71225476e-02 -6.29411414e-02\n",
            " -3.49417143e-02 -3.72454687e-03  1.67517923e-02  1.74221359e-02\n",
            "  8.67096111e-02 -5.21334670e-02 -9.95707512e-03 -1.62409935e-02\n",
            "  3.27702239e-02  3.58919427e-03 -8.23421478e-02 -1.64883211e-03\n",
            "  2.35430859e-02 -1.95483044e-02 -1.48018319e-02 -3.93047668e-02\n",
            "  2.78750677e-02  1.08272210e-02 -3.63175757e-02  2.45057344e-02\n",
            " -2.45150141e-02 -1.99477486e-02 -4.50449921e-02  5.69998957e-02\n",
            "  7.89616108e-02 -3.90553698e-02  3.14676613e-02  1.68070756e-02\n",
            "  4.17714380e-02  5.23959398e-02 -5.52481711e-02 -1.10786021e-01\n",
            " -2.96174120e-02 -3.86502780e-02 -4.10447381e-02  1.72210820e-02\n",
            "  6.14874698e-02 -8.61671288e-03 -2.69972570e-02 -3.49278860e-02\n",
            "  2.14060955e-02  6.54201508e-02  3.75608541e-02  8.62294622e-03\n",
            "  4.10126820e-02  3.75329480e-02 -4.54161316e-04 -7.46523589e-02\n",
            "  1.19725689e-02  8.63861069e-02 -1.56153310e-02 -3.10133281e-03\n",
            "  1.80034544e-02 -2.69964826e-03  2.36624274e-02 -1.77627653e-02\n",
            " -9.99894924e-03 -6.89931363e-02  5.49511947e-02 -8.18606745e-03\n",
            " -5.50479814e-02  5.67293055e-02  6.33748695e-02  3.78584117e-02\n",
            "  2.46352162e-02  6.17731363e-02  3.78466137e-02 -3.32530960e-02\n",
            " -1.30310711e-02  1.43337231e-02 -1.47374831e-02 -3.63188274e-02\n",
            "  1.19018769e-02 -1.16963871e-02 -5.23666590e-02  1.23278908e-02\n",
            " -3.40386033e-02  3.73916626e-02  2.44609006e-02 -5.71385324e-02\n",
            " -4.51593921e-02  3.68556678e-02  5.23006590e-03 -5.34112900e-02\n",
            " -7.49978656e-03 -4.40601446e-03 -7.52404891e-03 -2.76714079e-02\n",
            "  2.05516443e-03  4.17595878e-02 -1.59559529e-02  2.10030973e-02\n",
            " -3.47240642e-03 -9.44499485e-03 -8.22217837e-02  1.18041234e-02\n",
            "  3.88502777e-02 -1.99154858e-02 -1.32566810e-01 -1.15718339e-02\n",
            " -4.02939059e-02  1.99113935e-02 -2.49291994e-02  3.17915864e-02\n",
            "  5.21808453e-02  1.63629875e-02 -1.80177577e-02  4.20684479e-02\n",
            "  8.96415636e-02  3.64128873e-02  1.74677521e-02  1.05631840e-03\n",
            "  8.97897258e-02 -2.97745094e-02 -2.76565850e-02  6.59949798e-03\n",
            "  8.95437300e-02  4.94447537e-02  5.44326380e-02  5.68558648e-02\n",
            "  1.07445613e-01 -3.20494957e-02  1.88628994e-02 -3.72604206e-02\n",
            "  2.14162050e-03 -3.38861533e-02 -9.13295597e-02  5.71936974e-03\n",
            " -2.35068798e-02 -8.15444440e-02  5.17817624e-02  3.18397544e-02\n",
            " -9.82184429e-03  3.74551788e-02 -1.24099359e-01 -2.72542518e-02\n",
            "  1.72981191e-02 -1.76331811e-02 -6.66407123e-02  5.00974990e-02\n",
            " -4.69275042e-02 -8.53030756e-03 -4.68208715e-02  2.12671552e-02\n",
            "  4.21970487e-02  2.79996879e-02  5.53477276e-03  3.60253826e-02\n",
            "  1.63910333e-02 -3.07853781e-02 -2.90625580e-02  4.04237807e-02\n",
            " -4.98710871e-02  5.46280742e-02  7.08662719e-02 -3.33852619e-02\n",
            " -2.75859237e-03  1.33099724e-02  2.02648342e-03 -5.01098856e-02\n",
            "  2.45747324e-02  5.15799634e-02  1.78679917e-02 -6.27648979e-02\n",
            "  6.29340038e-02  1.86706558e-02  1.22492518e-02 -4.28927541e-02\n",
            " -6.07561832e-03 -1.98474545e-02 -6.50100410e-05 -9.90893319e-03\n",
            " -5.89036755e-03  2.44298019e-02  2.87071709e-02  8.89896676e-02\n",
            "  2.32130997e-02  2.09821835e-02 -1.16553633e-02 -2.56799851e-02\n",
            "  1.49275158e-02 -1.41543886e-02 -2.55170390e-02  2.21941993e-02\n",
            " -7.35114813e-02 -2.99032833e-02 -2.75981668e-02  3.32949385e-02\n",
            " -6.08316064e-02 -3.97252068e-02 -1.55551406e-02 -1.57969203e-02\n",
            " -5.52369654e-03 -2.69691944e-02 -1.04175881e-01 -7.95569196e-02\n",
            "  1.48095250e-01 -1.88155975e-02  3.76277044e-02  4.35854793e-02\n",
            "  3.75146754e-02 -1.28051132e-01  4.25304696e-02  4.62437347e-02\n",
            "  8.02811980e-02 -6.56051040e-02 -2.88130995e-02  7.91168399e-03\n",
            " -2.23641302e-02  2.61326954e-02  1.06501400e-01  9.86915082e-04\n",
            " -3.44330594e-02  1.84895354e-03 -1.41664147e-02  1.26424832e-02\n",
            " -3.42138037e-02  3.41970511e-02  4.34703566e-03 -1.09482929e-03\n",
            " -4.76906449e-02 -5.05028442e-02 -6.99414164e-02 -2.49411706e-02\n",
            " -2.77189538e-02 -3.19655128e-02  2.27888916e-02  5.49681857e-03\n",
            " -1.19039090e-02  4.84818779e-02  2.84778550e-02  3.67808267e-02\n",
            "  6.28819466e-02  7.00776577e-02 -9.78844836e-02 -1.67301893e-02\n",
            " -3.13598216e-02  9.44845937e-03  2.81970240e-02 -7.86999762e-02\n",
            " -1.07762162e-02 -5.06495796e-02  8.36703107e-02 -2.82265749e-02\n",
            " -4.29264903e-02  1.19034667e-03 -9.30781439e-02 -9.23760980e-03\n",
            "  1.51935816e-02  7.12248031e-03 -1.30619612e-02 -4.24892642e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#work white the labell"
      ],
      "metadata": {
        "id": "1ZpYFPpoBOPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_data_for_emotion(combined_df, emotion_columns, emotion, test_size=0.2, random_state=42):\n",
        "    # Separate the entries for the specific emotion\n",
        "    emotion_entries = combined_df[combined_df[emotion] == 1].copy()\n",
        "\n",
        "    # Set other emotion columns to 0\n",
        "    for col in emotion_columns:\n",
        "        if col != emotion:\n",
        "            emotion_entries[col] = 0\n",
        "\n",
        "    # Combine non-specific emotion entries\n",
        "    all_other_emotions = pd.concat([combined_df[combined_df[e] == 1] for e in emotion_columns if e != emotion])\n",
        "\n",
        "    # Handle potential list-type columns\n",
        "    for col in all_other_emotions.columns:\n",
        "        if all_other_emotions[col].apply(lambda x: isinstance(x, list)).any():\n",
        "            all_other_emotions[col] = all_other_emotions[col].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
        "\n",
        "    # Remove duplicates\n",
        "    all_other_emotions = all_other_emotions.drop_duplicates()\n",
        "\n",
        "    # Split data into train and test\n",
        "    train_emotion, test_emotion = train_test_split(emotion_entries, test_size=test_size, random_state=random_state)\n",
        "    train_other, test_other = train_test_split(all_other_emotions, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Filter to keep only processed_text and current emotion\n",
        "    train_emotion_filtered = train_emotion[['processed_text', emotion]]\n",
        "    test_emotion_filtered = test_emotion[['processed_text', emotion]]\n",
        "    train_other_filtered = train_other[['processed_text', emotion]]\n",
        "    test_other_filtered = test_other[['processed_text', emotion]]\n",
        "\n",
        "    # Combine emotion and non-emotion entries\n",
        "    train_combined = pd.concat([train_emotion_filtered, train_other_filtered], ignore_index=True)\n",
        "    test_combined = pd.concat([test_emotion_filtered, test_other_filtered], ignore_index=True)\n",
        "\n",
        "    print(f\"{emotion} - Train: {len(train_combined)}, Test: {len(test_combined)}\")\n",
        "\n",
        "    return train_combined, test_combined\n",
        "\n",
        "# Apply for all emotions\n",
        "emotion_data = {}\n",
        "for emotion in emotion_columns:\n",
        "    if emotion in combined_df.columns:\n",
        "        train, test = split_data_for_emotion(combined_df, emotion_columns, emotion)\n",
        "        emotion_data[emotion] = {'train': train, 'test': test}\n",
        "    else:\n",
        "        print(f\"Emotion {emotion} not found in the dataset.\")\n",
        "\n",
        "print(\"Data splitting complete for all emotions!\")\n"
      ],
      "metadata": {
        "id": "10ZeKI5FCWJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce25e60-8a45-4510-dfda-ebb743ae04b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "admiration - Train: 40942, Test: 10237\n",
            "amusement - Train: 39793, Test: 9949\n",
            "anger - Train: 39676, Test: 9919\n",
            "annoyance - Train: 40856, Test: 10215\n",
            "approval - Train: 41408, Test: 10353\n",
            "caring - Train: 39932, Test: 9983\n",
            "confusion - Train: 39764, Test: 9942\n",
            "curiosity - Train: 40650, Test: 10164\n",
            "desire - Train: 39538, Test: 9885\n",
            "disappointment - Train: 40091, Test: 10024\n",
            "disapproval - Train: 40248, Test: 10063\n",
            "disgust - Train: 39722, Test: 9931\n",
            "embarrassment - Train: 39174, Test: 9795\n",
            "excitement - Train: 39564, Test: 9891\n",
            "fear - Train: 39083, Test: 9772\n",
            "gratitude - Train: 39913, Test: 9979\n",
            "grief - Train: 38890, Test: 9724\n",
            "joy - Train: 39741, Test: 9937\n",
            "love - Train: 39618, Test: 9906\n",
            "nervousness - Train: 39034, Test: 9760\n",
            "optimism - Train: 40684, Test: 10172\n",
            "pride - Train: 39009, Test: 9754\n",
            "realization - Train: 41318, Test: 10330\n",
            "relief - Train: 38953, Test: 9739\n",
            "remorse - Train: 38994, Test: 9749\n",
            "sadness - Train: 39765, Test: 9942\n",
            "surprise - Train: 39513, Test: 9879\n",
            "neutral - Train: 38756, Test: 9690\n",
            "Data splitting complete for all emotions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_data.keys()"
      ],
      "metadata": {
        "id": "9ffFJ6e4DfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the data for input into the model"
      ],
      "metadata": {
        "id": "DDLIUZxwDnSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word_vectors, max_len=100):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word_vectors = word_vectors\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        emotion_label = self.labels[idx]\n",
        "\n",
        "        indexed_text = [self.word_vectors.get(word, np.zeros(300)) for word in text.split()]\n",
        "\n",
        "        indexed_text = indexed_text[:self.max_len] if len(indexed_text) > self.max_len else indexed_text + [np.zeros(300)] * (self.max_len - len(indexed_text))\n",
        "\n",
        "        indexed_text = torch.tensor(np.array(indexed_text), dtype=torch.float)\n",
        "        emotion_label = torch.tensor(emotion_label, dtype=torch.float)\n",
        "\n",
        "        return indexed_text, emotion_label\n",
        "\n",
        "# Dictionary to store dataloaders for each emotion\n",
        "emotion_dataloaders = {}\n",
        "\n",
        "k = 5\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "for emotion in emotion_columns:\n",
        "    train_texts = emotion_data[emotion]['train']['processed_text'].tolist()\n",
        "    train_labels = emotion_data[emotion]['train'][emotion].tolist()\n",
        "\n",
        "    test_texts = emotion_data[emotion]['test']['processed_text'].tolist()\n",
        "    test_labels = emotion_data[emotion]['test'][emotion].tolist()\n",
        "\n",
        "    test_dataset = EmotionDataset(test_texts, test_labels, word_vectors)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    emotion_dataloaders[emotion] = {'test': test_dataloader}\n",
        "\n",
        "    X = np.array(train_texts)\n",
        "    y = np.array(train_labels)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        print(f\"{emotion} - Training on Fold {fold+1}...\")\n",
        "\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        train_dataset_fold = EmotionDataset(X_train_fold, y_train_fold, word_vectors)\n",
        "        val_dataset_fold = EmotionDataset(X_val_fold, y_val_fold, word_vectors)\n",
        "\n",
        "        train_dataloader_fold = DataLoader(train_dataset_fold, batch_size=32, shuffle=True)\n",
        "        val_dataloader_fold = DataLoader(val_dataset_fold, batch_size=32, shuffle=False)\n",
        "\n",
        "        if 'folds' not in emotion_dataloaders[emotion]:\n",
        "            emotion_dataloaders[emotion]['folds'] = []\n",
        "\n",
        "        emotion_dataloaders[emotion]['folds'].append({'train': train_dataloader_fold, 'val': val_dataloader_fold})\n",
        "\n",
        "print(\"Dataloaders ready for all emotions!\")\n"
      ],
      "metadata": {
        "id": "BotRH0bDDwRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cec4134-7280-449a-91bc-06d1b86a8d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "admiration - Training on Fold 1...\n",
            "admiration - Training on Fold 2...\n",
            "admiration - Training on Fold 3...\n",
            "admiration - Training on Fold 4...\n",
            "admiration - Training on Fold 5...\n",
            "amusement - Training on Fold 1...\n",
            "amusement - Training on Fold 2...\n",
            "amusement - Training on Fold 3...\n",
            "amusement - Training on Fold 4...\n",
            "amusement - Training on Fold 5...\n",
            "anger - Training on Fold 1...\n",
            "anger - Training on Fold 2...\n",
            "anger - Training on Fold 3...\n",
            "anger - Training on Fold 4...\n",
            "anger - Training on Fold 5...\n",
            "annoyance - Training on Fold 1...\n",
            "annoyance - Training on Fold 2...\n",
            "annoyance - Training on Fold 3...\n",
            "annoyance - Training on Fold 4...\n",
            "annoyance - Training on Fold 5...\n",
            "approval - Training on Fold 1...\n",
            "approval - Training on Fold 2...\n",
            "approval - Training on Fold 3...\n",
            "approval - Training on Fold 4...\n",
            "approval - Training on Fold 5...\n",
            "caring - Training on Fold 1...\n",
            "caring - Training on Fold 2...\n",
            "caring - Training on Fold 3...\n",
            "caring - Training on Fold 4...\n",
            "caring - Training on Fold 5...\n",
            "confusion - Training on Fold 1...\n",
            "confusion - Training on Fold 2...\n",
            "confusion - Training on Fold 3...\n",
            "confusion - Training on Fold 4...\n",
            "confusion - Training on Fold 5...\n",
            "curiosity - Training on Fold 1...\n",
            "curiosity - Training on Fold 2...\n",
            "curiosity - Training on Fold 3...\n",
            "curiosity - Training on Fold 4...\n",
            "curiosity - Training on Fold 5...\n",
            "desire - Training on Fold 1...\n",
            "desire - Training on Fold 2...\n",
            "desire - Training on Fold 3...\n",
            "desire - Training on Fold 4...\n",
            "desire - Training on Fold 5...\n",
            "disappointment - Training on Fold 1...\n",
            "disappointment - Training on Fold 2...\n",
            "disappointment - Training on Fold 3...\n",
            "disappointment - Training on Fold 4...\n",
            "disappointment - Training on Fold 5...\n",
            "disapproval - Training on Fold 1...\n",
            "disapproval - Training on Fold 2...\n",
            "disapproval - Training on Fold 3...\n",
            "disapproval - Training on Fold 4...\n",
            "disapproval - Training on Fold 5...\n",
            "disgust - Training on Fold 1...\n",
            "disgust - Training on Fold 2...\n",
            "disgust - Training on Fold 3...\n",
            "disgust - Training on Fold 4...\n",
            "disgust - Training on Fold 5...\n",
            "embarrassment - Training on Fold 1...\n",
            "embarrassment - Training on Fold 2...\n",
            "embarrassment - Training on Fold 3...\n",
            "embarrassment - Training on Fold 4...\n",
            "embarrassment - Training on Fold 5...\n",
            "excitement - Training on Fold 1...\n",
            "excitement - Training on Fold 2...\n",
            "excitement - Training on Fold 3...\n",
            "excitement - Training on Fold 4...\n",
            "excitement - Training on Fold 5...\n",
            "fear - Training on Fold 1...\n",
            "fear - Training on Fold 2...\n",
            "fear - Training on Fold 3...\n",
            "fear - Training on Fold 4...\n",
            "fear - Training on Fold 5...\n",
            "gratitude - Training on Fold 1...\n",
            "gratitude - Training on Fold 2...\n",
            "gratitude - Training on Fold 3...\n",
            "gratitude - Training on Fold 4...\n",
            "gratitude - Training on Fold 5...\n",
            "grief - Training on Fold 1...\n",
            "grief - Training on Fold 2...\n",
            "grief - Training on Fold 3...\n",
            "grief - Training on Fold 4...\n",
            "grief - Training on Fold 5...\n",
            "joy - Training on Fold 1...\n",
            "joy - Training on Fold 2...\n",
            "joy - Training on Fold 3...\n",
            "joy - Training on Fold 4...\n",
            "joy - Training on Fold 5...\n",
            "love - Training on Fold 1...\n",
            "love - Training on Fold 2...\n",
            "love - Training on Fold 3...\n",
            "love - Training on Fold 4...\n",
            "love - Training on Fold 5...\n",
            "nervousness - Training on Fold 1...\n",
            "nervousness - Training on Fold 2...\n",
            "nervousness - Training on Fold 3...\n",
            "nervousness - Training on Fold 4...\n",
            "nervousness - Training on Fold 5...\n",
            "optimism - Training on Fold 1...\n",
            "optimism - Training on Fold 2...\n",
            "optimism - Training on Fold 3...\n",
            "optimism - Training on Fold 4...\n",
            "optimism - Training on Fold 5...\n",
            "pride - Training on Fold 1...\n",
            "pride - Training on Fold 2...\n",
            "pride - Training on Fold 3...\n",
            "pride - Training on Fold 4...\n",
            "pride - Training on Fold 5...\n",
            "realization - Training on Fold 1...\n",
            "realization - Training on Fold 2...\n",
            "realization - Training on Fold 3...\n",
            "realization - Training on Fold 4...\n",
            "realization - Training on Fold 5...\n",
            "relief - Training on Fold 1...\n",
            "relief - Training on Fold 2...\n",
            "relief - Training on Fold 3...\n",
            "relief - Training on Fold 4...\n",
            "relief - Training on Fold 5...\n",
            "remorse - Training on Fold 1...\n",
            "remorse - Training on Fold 2...\n",
            "remorse - Training on Fold 3...\n",
            "remorse - Training on Fold 4...\n",
            "remorse - Training on Fold 5...\n",
            "sadness - Training on Fold 1...\n",
            "sadness - Training on Fold 2...\n",
            "sadness - Training on Fold 3...\n",
            "sadness - Training on Fold 4...\n",
            "sadness - Training on Fold 5...\n",
            "surprise - Training on Fold 1...\n",
            "surprise - Training on Fold 2...\n",
            "surprise - Training on Fold 3...\n",
            "surprise - Training on Fold 4...\n",
            "surprise - Training on Fold 5...\n",
            "neutral - Training on Fold 1...\n",
            "neutral - Training on Fold 2...\n",
            "neutral - Training on Fold 3...\n",
            "neutral - Training on Fold 4...\n",
            "neutral - Training on Fold 5...\n",
            "Dataloaders ready for all emotions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EmotionLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim=300, hidden_dim=128, output_dim=1, n_layers=2, dropout=0.5):\n",
        "        super(EmotionLSTMWithAttention, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(dropout)\n",
        "        # Bidirectional LSTM Layer\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)  # Attention mechanism to calculate attention scores\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        # LayerNorm after LSTM\n",
        "        self.ln = nn.LayerNorm(hidden_dim * 2)  # Since bidirectional, we multiply hidden_dim by 2\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "        # Dropout after the LSTM and fully connected layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def attention_mechanism(self, lstm_output):\n",
        "        \"\"\"\n",
        "        Applies attention mechanism over the LSTM outputs.\n",
        "        \"\"\"\n",
        "        attention_scores = self.attention(lstm_output)  # Shape: (batch_size, seq_len, 1)\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)  # Shape: (batch_size, seq_len, 1)\n",
        "        weighted_sum = torch.sum(attention_weights * lstm_output, dim=1)  # Shape: (batch_size, hidden_dim*2)\n",
        "        weighted_sum = self.attn_dropout(weighted_sum)\n",
        "        return weighted_sum\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is the input tensor of shape (batch_size, seq_len, input_dim)\n",
        "        x = self.input_dropout(x)\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_out = self.attention_mechanism(lstm_out)\n",
        "\n",
        "        # Layer normalization\n",
        "        attention_out = self.ln(attention_out)\n",
        "\n",
        "        # Pass through fully connected layer and dropout\n",
        "        out = self.fc(attention_out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Instantiate the model with the desired parameters\n",
        "emotion_models = {}\n",
        "\n",
        "for emotion in emotion_columns:\n",
        "    model = EmotionLSTMWithAttention(input_dim=300, hidden_dim=128, output_dim=1, n_layers=2, dropout=0.5)\n",
        "    emotion_models[emotion] = model"
      ],
      "metadata": {
        "id": "Hh76ygQBBcFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#white Head attention"
      ],
      "metadata": {
        "id": "2eQ-KLxorJfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EmotionLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim=300, hidden_dim=128, output_dim=1, n_layers=2, dropout=0.5,num_heads = 8):\n",
        "        super(EmotionLSTMWithAttention, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(dropout)\n",
        "        # Bidirectional LSTM Layer\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.ln_lstm = nn.LayerNorm(hidden_dim * 2)\n",
        "\n",
        "        self.multihead_attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=num_heads, dropout=dropout)  # Added multi-head attention\n",
        "\n",
        "        # Attention Mechanism\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)  # Attention mechanism to calculate attention scores\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        # LayerNorm after LSTM\n",
        "        self.ln = nn.LayerNorm(hidden_dim * 2)  # Since bidirectional, we multiply hidden_dim by 2\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "        # Dropout after the LSTM and fully connected layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pos_weight = None\n",
        "\n",
        "\n",
        "    def attention_mechanism(self, lstm_output):\n",
        "        \"\"\"\n",
        "        Applies attention mechanism over the LSTM outputs.\n",
        "        \"\"\"\n",
        "        attention_scores = self.attention(lstm_output)  # Shape: (batch_size, seq_len, 1)\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)  # Shape: (batch_size, seq_len, 1)\n",
        "        weighted_sum = torch.sum(attention_weights * lstm_output, dim=1)  # Shape: (batch_size, hidden_dim*2)\n",
        "        weighted_sum = self.attn_dropout(weighted_sum)\n",
        "        return weighted_sum\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is the input tensor of shape (batch_size, seq_len, input_dim)\n",
        "        x = self.input_dropout(x)\n",
        "        # LSTM forward pass\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Apply LayerNorm after LSTM\n",
        "        lstm_out = self.ln_lstm(lstm_out)\n",
        "\n",
        "        # Apply Multi-Head Attention\n",
        "        attn_out, _ = self.multihead_attention(lstm_out, lstm_out, lstm_out)\n",
        "\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_out = self.attention_mechanism(attn_out)\n",
        "\n",
        "        # Layer normalization\n",
        "        attention_out = self.ln(attention_out)\n",
        "\n",
        "        # Pass through fully connected layer and dropout\n",
        "        out = self.fc(attention_out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Instantiate the model with the desired parameters\n",
        "emotion_models = {}\n",
        "\n",
        "for emotion in emotion_columns:\n",
        "    model = EmotionLSTMWithAttention(input_dim=300, hidden_dim=128, output_dim=1, n_layers=2, dropout=0.3)\n",
        "    emotion_models[emotion] = model"
      ],
      "metadata": {
        "id": "QfQDgK7oqv93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Focal Loss Implementation\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, reduction='mean', pos_weight=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight, reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "\n",
        "        return F_loss.mean() if self.reduction == 'mean' else F_loss.sum()\n",
        "\n",
        "# Training function with additional metrics, early stopping, and LR scheduler\n",
        "def simple_train(model, train_dataloader_fold, val_dataloader_fold, criterion, optimizer, epochs=5):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    val_precisions = []\n",
        "    val_recalls = []\n",
        "    val_f1s = []\n",
        "    val_aucs = []\n",
        "    val_pr_auc = []\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    patience = 3\n",
        "\n",
        "    model.to(device)\n",
        "    writer = SummaryWriter('runs/emotion_model')\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "        epoch_train_acc = 0\n",
        "\n",
        "        for inputs, labels in train_dataloader_fold:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            labels = labels.float()\n",
        "            loss = criterion(outputs.view(-1), labels)\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            preds = torch.round(torch.sigmoid(outputs)).detach().cpu().numpy()\n",
        "            batch_acc = f1_score(labels.detach().cpu().numpy(), preds, average=\"macro\", zero_division=0)\n",
        "            epoch_train_acc += batch_acc\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / len(train_dataloader_fold)\n",
        "        avg_train_acc = epoch_train_acc / len(train_dataloader_fold)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuracies.append(avg_train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0\n",
        "        epoch_val_acc = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_dataloader_fold:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                labels = labels.float()\n",
        "                loss = criterion(outputs.view(-1), labels)\n",
        "                epoch_val_loss += loss.item()\n",
        "\n",
        "                preds = torch.round(torch.sigmoid(outputs)).detach().cpu().numpy()\n",
        "                batch_acc = f1_score(labels.detach().cpu().numpy(), preds, average=\"macro\", zero_division=0)\n",
        "                epoch_val_acc += batch_acc\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "        avg_val_loss = epoch_val_loss / len(val_dataloader_fold)\n",
        "        avg_val_acc = epoch_val_acc / len(val_dataloader_fold)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(avg_val_acc)\n",
        "\n",
        "        epoch_val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        epoch_val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        epoch_val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        try:\n",
        "            epoch_val_auc = roc_auc_score(all_labels, all_preds)\n",
        "        except Exception:\n",
        "            epoch_val_auc = 0.5\n",
        "\n",
        "        val_f1s.append(epoch_val_f1)\n",
        "        val_precisions.append(epoch_val_precision)\n",
        "        val_recalls.append(epoch_val_recall)\n",
        "        val_aucs.append(epoch_val_auc)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f}s - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), f'best_model_{label}.pt')\n",
        "            model.to(device)\n",
        "            print(\"  Saved the best model!\")\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\" ⏹️ Early stopping triggered. Stopping training.\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, val_precisions, val_recalls, val_f1s, val_aucs\n",
        "\n",
        "# Training loop for all emotion models\n",
        "for label, model in emotion_models.items():\n",
        "    print(f\"Starting training for emotion: {label}\")\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    train_labels = emotion_data[label]['train'][label].tolist()\n",
        "    val_labels = emotion_data[label]['test'][label].tolist()\n",
        "\n",
        "    train_texts = emotion_data[label]['train']['processed_text'].tolist()\n",
        "    val_texts = emotion_data[label]['test']['processed_text'].tolist()\n",
        "\n",
        "    label_counts = np.bincount(train_labels)\n",
        "\n",
        "    if len(label_counts) < 2:\n",
        "        print(f\"⚠️  Skipping training for {label} (only one class present)\")\n",
        "        continue\n",
        "\n",
        "    weights = 1.0 / label_counts\n",
        "    sample_weights = weights[train_labels]\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    # pos_count = np.sum(train_labels)\n",
        "    # neg_count = len(train_labels) - pos_count\n",
        "    # pos_weight = torch.tensor([neg_count / max(pos_count, 1)]).to(device)\n",
        "\n",
        "    pos_weight = torch.tensor([len(train_labels) - np.sum(train_labels)] / max(np.sum(train_labels), 1)).to(device)\n",
        "\n",
        "    criterion = FocalLoss(alpha=0.20, gamma=2.5, pos_weight=pos_weight)\n",
        "    train_dataset = EmotionDataset(train_texts, train_labels, word_vectors)\n",
        "    val_dataset = EmotionDataset(val_texts, val_labels, word_vectors)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    patience = 3\n",
        "\n",
        "    results = simple_train(model, train_dataloader, val_dataloader, criterion, optimizer, epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "id": "eNeTQyH1FLBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "xuOkPzSafbe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "os.makedirs(\"confusion_matrices\", exist_ok=True)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, emotion):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False, linewidths=1, linecolor='gray')\n",
        "\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title(f'Confusion Matrix for {emotion}')\n",
        "\n",
        "    plt.savefig(f'confusion_matrices/confusion_matrix_{emotion}.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "def evaluate_model_on_test(model, test_dataloader, emotion):\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    f1_list = []\n",
        "    accuracy_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.round(torch.sigmoid(outputs)).cpu().numpy()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds)\n",
        "\n",
        "            precision = precision_score(labels.cpu().numpy(), preds, zero_division=1)\n",
        "            recall = recall_score(labels.cpu().numpy(), preds, zero_division=1)\n",
        "            f1 = f1_score(labels.cpu().numpy(), preds, zero_division=1)\n",
        "            accuracy = accuracy_score(labels.cpu().numpy(), preds)\n",
        "\n",
        "            precision_list.append(precision)\n",
        "            recall_list.append(recall)\n",
        "            f1_list.append(f1)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "    print(f\"Evaluation results for {emotion}:\")\n",
        "    print(f\"Precision: {np.mean(precision_list):.4f}\")\n",
        "    print(f\"Recall: {np.mean(recall_list):.4f}\")\n",
        "    print(f\"F1 Score: {np.mean(f1_list):.4f}\")\n",
        "    print(f\"Accuracy: {np.mean(accuracy_list):.4f}\")\n",
        "\n",
        "    plot_confusion_matrix(np.array(all_labels), np.array(all_preds), emotion)\n",
        "\n",
        "    return {\n",
        "        'precision': np.mean(precision_list),\n",
        "        'recall': np.mean(recall_list),\n",
        "        'f1_score': np.mean(f1_list),\n",
        "        'accuracy': np.mean(accuracy_list)\n",
        "    }\n",
        "\n",
        "all_metrics = {}\n",
        "\n",
        "for emotion in emotion_columns:\n",
        "    if emotion in combined_df.columns:\n",
        "        train, test = split_data_for_emotion(combined_df, emotion_columns, emotion)\n",
        "\n",
        "        test_dataloader = DataLoader(EmotionDataset(test['processed_text'].tolist(), test[emotion].tolist(), word_vectors), batch_size=32, shuffle=False)\n",
        "\n",
        "        print(f\"Evaluating model for {emotion}...\")\n",
        "        metrics = evaluate_model_on_test(trained_models[emotion], test_dataloader, emotion)\n",
        "\n",
        "        all_metrics[emotion] = metrics\n",
        "    else:\n",
        "        print(f\"Emotion {emotion} not found in the dataset.\")\n",
        "\n",
        "print(\"Evaluation complete for all emotions!\")\n",
        "\n",
        "def plot_comparison_graph(metrics_dict):\n",
        "    emotions = list(metrics_dict.keys())\n",
        "    precision = [metrics_dict[emotion]['precision'] for emotion in emotions]\n",
        "    recall = [metrics_dict[emotion]['recall'] for emotion in emotions]\n",
        "    f1 = [metrics_dict[emotion]['f1_score'] for emotion in emotions]\n",
        "    accuracy = [metrics_dict[emotion]['accuracy'] for emotion in emotions]\n",
        "\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    ax[0, 0].barh(emotions, precision, color='lightblue')\n",
        "    ax[0, 0].set_title('Precision per Emotion')\n",
        "\n",
        "    ax[0, 1].barh(emotions, recall, color='lightgreen')\n",
        "    ax[0, 1].set_title('Recall per Emotion')\n",
        "\n",
        "    ax[1, 0].barh(emotions, f1, color='lightcoral')\n",
        "    ax[1, 0].set_title('F1 Score per Emotion')\n",
        "\n",
        "    ax[1, 1].barh(emotions, accuracy, color='lightyellow')\n",
        "    ax[1, 1].set_title('Accuracy per Emotion')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices/model_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "plot_comparison_graph(all_metrics)\n"
      ],
      "metadata": {
        "id": "6ou4hFFDfa3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ace_tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmLYVD5xn7AL",
        "outputId": "427bb042-af35-43a2-ff9e-7730563caa8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ace_tools\n",
            "  Downloading ace_tools-0.0-py3-none-any.whl.metadata (300 bytes)\n",
            "Downloading ace_tools-0.0-py3-none-any.whl (1.1 kB)\n",
            "Installing collected packages: ace_tools\n",
            "Successfully installed ace_tools-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "emotion_models = {emotion: None for emotion in emotion_columns}\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def load_trained_models(emotion_models, model_class, device):\n",
        "    trained_models = {}\n",
        "    for emotion in emotion_models.keys():\n",
        "        model_filename = f'best_model_{emotion}.pt'\n",
        "        if os.path.exists(model_filename):\n",
        "            model = model_class()\n",
        "            model.load_state_dict(torch.load(model_filename, map_location=device))\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "            trained_models[emotion] = model\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: Model file {model_filename} not found, skipping {emotion}\")\n",
        "\n",
        "    return trained_models\n",
        "\n",
        "def predict_ensemble(text, trained_models, word_vectors, device):\n",
        "    text_vectorized = [word_vectors.get(word, np.zeros(300)) for word in text.split()]\n",
        "    text_vectorized = text_vectorized[:100] if len(text_vectorized) > 100 else text_vectorized + [np.zeros(300)] * (100 - len(text_vectorized))\n",
        "    text_tensor = torch.tensor(np.array(text_vectorized), dtype=torch.float).unsqueeze(0).to(device)\n",
        "\n",
        "    emotion_probs = {}\n",
        "    for emotion, model in trained_models.items():\n",
        "        with torch.no_grad():\n",
        "            output = model(text_tensor)\n",
        "            prob = torch.sigmoid(output).item()\n",
        "            emotion_probs[emotion] = prob\n",
        "\n",
        "    total_prob = sum(emotion_probs.values())\n",
        "    for emotion in emotion_probs:\n",
        "        emotion_probs[emotion] /= total_prob if total_prob > 0 else 1\n",
        "\n",
        "    df = pd.DataFrame(list(emotion_probs.items()), columns=['Emotion', 'Probability'])\n",
        "    df = df.sort_values(by='Probability', ascending=False)\n",
        "    return df\n",
        "\n",
        "text = \"I feel so sad\"\n",
        "trained_models = load_trained_models(emotion_models, EmotionLSTMWithAttention, device)\n",
        "result_df = predict_ensemble(text, trained_models, word_vectors, device)\n",
        "display(result_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "lb2zfuJNV9Y_",
        "outputId": "583e2b53-428d-4398-de32-ac0a83d22740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Emotion  Probability\n",
              "16           grief     0.048088\n",
              "24         remorse     0.047397\n",
              "25         sadness     0.047073\n",
              "14            fear     0.047000\n",
              "23          relief     0.046654\n",
              "19     nervousness     0.046477\n",
              "12   embarrassment     0.046186\n",
              "17             joy     0.043750\n",
              "11         disgust     0.043409\n",
              "13      excitement     0.042442\n",
              "9   disappointment     0.038526\n",
              "5           caring     0.038389\n",
              "26        surprise     0.037705\n",
              "27         neutral     0.035936\n",
              "2            anger     0.035315\n",
              "3        annoyance     0.034298\n",
              "0       admiration     0.032689\n",
              "1        amusement     0.032202\n",
              "4         approval     0.031459\n",
              "20        optimism     0.029551\n",
              "22     realization     0.029143\n",
              "18            love     0.028479\n",
              "8           desire     0.027091\n",
              "15       gratitude     0.026203\n",
              "7        curiosity     0.024466\n",
              "10     disapproval     0.024220\n",
              "6        confusion     0.022398\n",
              "21           pride     0.013454"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99c9360c-c4df-4d6e-9889-88b98e35a7e0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>grief</td>\n",
              "      <td>0.048088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>remorse</td>\n",
              "      <td>0.047397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>sadness</td>\n",
              "      <td>0.047073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>fear</td>\n",
              "      <td>0.047000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>relief</td>\n",
              "      <td>0.046654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>nervousness</td>\n",
              "      <td>0.046477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>embarrassment</td>\n",
              "      <td>0.046186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>joy</td>\n",
              "      <td>0.043750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>disgust</td>\n",
              "      <td>0.043409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>excitement</td>\n",
              "      <td>0.042442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>disappointment</td>\n",
              "      <td>0.038526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>caring</td>\n",
              "      <td>0.038389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>surprise</td>\n",
              "      <td>0.037705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>neutral</td>\n",
              "      <td>0.035936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>0.035315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>annoyance</td>\n",
              "      <td>0.034298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>admiration</td>\n",
              "      <td>0.032689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>amusement</td>\n",
              "      <td>0.032202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>approval</td>\n",
              "      <td>0.031459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>optimism</td>\n",
              "      <td>0.029551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>realization</td>\n",
              "      <td>0.029143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>love</td>\n",
              "      <td>0.028479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>desire</td>\n",
              "      <td>0.027091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gratitude</td>\n",
              "      <td>0.026203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>curiosity</td>\n",
              "      <td>0.024466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>disapproval</td>\n",
              "      <td>0.024220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>confusion</td>\n",
              "      <td>0.022398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>pride</td>\n",
              "      <td>0.013454</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99c9360c-c4df-4d6e-9889-88b98e35a7e0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99c9360c-c4df-4d6e-9889-88b98e35a7e0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99c9360c-c4df-4d6e-9889-88b98e35a7e0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8d9fd7bf-f342-4af7-8873-bd65c7fefe9d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d9fd7bf-f342-4af7-8873-bd65c7fefe9d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8d9fd7bf-f342-4af7-8873-bd65c7fefe9d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d374d03e-6b06-4fe4-90a5-78678a84ceb8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d374d03e-6b06-4fe4-90a5-78678a84ceb8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result_df",
              "summary": "{\n  \"name\": \"result_df\",\n  \"rows\": 28,\n  \"fields\": [\n    {\n      \"column\": \"Emotion\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"excitement\",\n          \"disapproval\",\n          \"disgust\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009331050316256856,\n        \"min\": 0.013453672711027582,\n        \"max\": 0.048088053951977405,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          0.04244248452975192,\n          0.024220349017101038,\n          0.04340858657654674\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test white a new text's"
      ],
      "metadata": {
        "id": "x6OvwIosr8Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "texts = [\n",
        "    #Negative emotions\n",
        "    \"I feel so sad\",\n",
        "    \"This is the worst day of my life\",\n",
        "    \"I'm feeling angry and frustrated\",\n",
        "    \"Everything seems hopeless\",\n",
        "    \"I'm so disappointed in myself\",\n",
        "\n",
        "    #Positive emotions\n",
        "    \"I'm so happy today!\",\n",
        "    \"I feel loved and appreciated\",\n",
        "    \"This is the best day ever\",\n",
        "    \"I'm really excited about the future\",\n",
        "    \"I feel so grateful for my friends\",\n",
        "\n",
        "    # Mixed emotions\n",
        "    \"I'm excited but also nervous about the presentation\",\n",
        "    \"I'm happy to see you, but also feel a bit sad\",\n",
        "    \"I feel proud but anxious about the exam results\",\n",
        "    \"I'm happy but also a little scared about the changes\",\n",
        "    \"I’m feeling joyful and overwhelmed at the same time\",\n",
        "\n",
        "    # Neutral emotions\n",
        "    \"I just woke up\",\n",
        "    \"The weather is fine today\",\n",
        "    \"I need to go to work\",\n",
        "    \"The meeting is scheduled for 3 PM\",\n",
        "    \"I'm reading a book right now\",\n",
        "\n",
        "    #Subtle emotions\n",
        "    \"I don’t know what to feel about this\",\n",
        "    \"It’s hard to say how I feel\",\n",
        "    \"I have mixed feelings about this\",\n",
        "    \"I’m trying to understand my emotions\",\n",
        "    \"It’s complicated\"\n",
        "]\n",
        "\n",
        "\n",
        "trained_models = load_trained_models(emotion_models, EmotionLSTMWithAttention, device)\n",
        "\n",
        "for text in texts:\n",
        "    result_df = predict_ensemble(text, trained_models, word_vectors, device)\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(result_df)\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGXn-EMNr_wz",
        "outputId": "04240a03-94a5-4a37-e6ba-7060f5370256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'I feel so sad'\n",
            "           Emotion  Probability\n",
            "16           grief     0.048088\n",
            "24         remorse     0.047397\n",
            "25         sadness     0.047073\n",
            "14            fear     0.047000\n",
            "23          relief     0.046654\n",
            "19     nervousness     0.046477\n",
            "12   embarrassment     0.046186\n",
            "17             joy     0.043750\n",
            "11         disgust     0.043409\n",
            "13      excitement     0.042442\n",
            "9   disappointment     0.038526\n",
            "5           caring     0.038389\n",
            "26        surprise     0.037705\n",
            "27         neutral     0.035936\n",
            "2            anger     0.035315\n",
            "3        annoyance     0.034298\n",
            "0       admiration     0.032689\n",
            "1        amusement     0.032202\n",
            "4         approval     0.031459\n",
            "20        optimism     0.029551\n",
            "22     realization     0.029143\n",
            "18            love     0.028479\n",
            "8           desire     0.027091\n",
            "15       gratitude     0.026203\n",
            "7        curiosity     0.024466\n",
            "10     disapproval     0.024220\n",
            "6        confusion     0.022398\n",
            "21           pride     0.013454\n",
            "==================================================\n",
            "Text: 'This is the worst day of my life'\n",
            "           Emotion  Probability\n",
            "16           grief     0.046075\n",
            "14            fear     0.045569\n",
            "21           pride     0.045087\n",
            "23          relief     0.044961\n",
            "19     nervousness     0.044868\n",
            "11         disgust     0.043753\n",
            "12   embarrassment     0.043209\n",
            "13      excitement     0.041232\n",
            "25         sadness     0.041022\n",
            "18            love     0.039550\n",
            "17             joy     0.039465\n",
            "9   disappointment     0.037755\n",
            "2            anger     0.037666\n",
            "1        amusement     0.036049\n",
            "26        surprise     0.035870\n",
            "0       admiration     0.033887\n",
            "27         neutral     0.033772\n",
            "3        annoyance     0.033396\n",
            "22     realization     0.032629\n",
            "20        optimism     0.031910\n",
            "4         approval     0.031823\n",
            "5           caring     0.031120\n",
            "7        curiosity     0.026890\n",
            "8           desire     0.026232\n",
            "15       gratitude     0.024566\n",
            "10     disapproval     0.024429\n",
            "24         remorse     0.024176\n",
            "6        confusion     0.023039\n",
            "==================================================\n",
            "Text: 'I'm feeling angry and frustrated'\n",
            "           Emotion  Probability\n",
            "14            fear     0.050845\n",
            "19     nervousness     0.050188\n",
            "24         remorse     0.049775\n",
            "12   embarrassment     0.049193\n",
            "16           grief     0.048145\n",
            "25         sadness     0.047702\n",
            "11         disgust     0.046561\n",
            "13      excitement     0.043119\n",
            "2            anger     0.042041\n",
            "27         neutral     0.040064\n",
            "9   disappointment     0.039767\n",
            "26        surprise     0.038997\n",
            "3        annoyance     0.038450\n",
            "17             joy     0.037976\n",
            "5           caring     0.037874\n",
            "4         approval     0.033468\n",
            "10     disapproval     0.033148\n",
            "0       admiration     0.029759\n",
            "22     realization     0.029545\n",
            "20        optimism     0.029356\n",
            "23          relief     0.028338\n",
            "1        amusement     0.026473\n",
            "7        curiosity     0.026250\n",
            "6        confusion     0.026088\n",
            "15       gratitude     0.024664\n",
            "18            love     0.020040\n",
            "8           desire     0.018177\n",
            "21           pride     0.013998\n",
            "==================================================\n",
            "Text: 'Everything seems hopeless'\n",
            "           Emotion  Probability\n",
            "14            fear     0.048624\n",
            "13      excitement     0.048600\n",
            "25         sadness     0.047284\n",
            "27         neutral     0.044992\n",
            "12   embarrassment     0.044847\n",
            "11         disgust     0.044716\n",
            "9   disappointment     0.043786\n",
            "2            anger     0.043731\n",
            "26        surprise     0.042934\n",
            "17             joy     0.038468\n",
            "4         approval     0.037995\n",
            "0       admiration     0.036886\n",
            "19     nervousness     0.036609\n",
            "10     disapproval     0.035948\n",
            "6        confusion     0.035418\n",
            "20        optimism     0.034149\n",
            "24         remorse     0.034108\n",
            "22     realization     0.033402\n",
            "5           caring     0.032310\n",
            "7        curiosity     0.031921\n",
            "3        annoyance     0.031776\n",
            "1        amusement     0.029027\n",
            "15       gratitude     0.028828\n",
            "23          relief     0.026493\n",
            "16           grief     0.025199\n",
            "18            love     0.024245\n",
            "8           desire     0.021403\n",
            "21           pride     0.016301\n",
            "==================================================\n",
            "Text: 'I'm so disappointed in myself'\n",
            "           Emotion  Probability\n",
            "23          relief     0.050725\n",
            "24         remorse     0.049921\n",
            "14            fear     0.049598\n",
            "13      excitement     0.046994\n",
            "12   embarrassment     0.045230\n",
            "25         sadness     0.044967\n",
            "26        surprise     0.043800\n",
            "17             joy     0.042891\n",
            "27         neutral     0.040779\n",
            "9   disappointment     0.040486\n",
            "2            anger     0.038317\n",
            "0       admiration     0.035432\n",
            "4         approval     0.034931\n",
            "3        annoyance     0.034100\n",
            "19     nervousness     0.033159\n",
            "22     realization     0.032826\n",
            "20        optimism     0.032328\n",
            "6        confusion     0.032132\n",
            "7        curiosity     0.031981\n",
            "5           caring     0.030981\n",
            "1        amusement     0.029161\n",
            "15       gratitude     0.028281\n",
            "21           pride     0.026548\n",
            "11         disgust     0.026064\n",
            "10     disapproval     0.025716\n",
            "8           desire     0.025557\n",
            "18            love     0.024330\n",
            "16           grief     0.022764\n",
            "==================================================\n",
            "Text: 'I'm so happy today!'\n",
            "           Emotion  Probability\n",
            "23          relief     0.053719\n",
            "17             joy     0.051853\n",
            "13      excitement     0.051848\n",
            "27         neutral     0.044478\n",
            "0       admiration     0.044276\n",
            "26        surprise     0.043920\n",
            "9   disappointment     0.043237\n",
            "5           caring     0.040307\n",
            "2            anger     0.040109\n",
            "25         sadness     0.039378\n",
            "15       gratitude     0.038667\n",
            "4         approval     0.038163\n",
            "18            love     0.037246\n",
            "20        optimism     0.035729\n",
            "12   embarrassment     0.034413\n",
            "21           pride     0.034403\n",
            "22     realization     0.033792\n",
            "1        amusement     0.032420\n",
            "7        curiosity     0.031851\n",
            "24         remorse     0.031795\n",
            "14            fear     0.030874\n",
            "8           desire     0.030490\n",
            "6        confusion     0.029003\n",
            "3        annoyance     0.025814\n",
            "19     nervousness     0.022450\n",
            "16           grief     0.021181\n",
            "11         disgust     0.020944\n",
            "10     disapproval     0.017638\n",
            "==================================================\n",
            "Text: 'I feel loved and appreciated'\n",
            "           Emotion  Probability\n",
            "18            love     0.051891\n",
            "17             joy     0.050055\n",
            "13      excitement     0.048861\n",
            "5           caring     0.046101\n",
            "25         sadness     0.045996\n",
            "0       admiration     0.045118\n",
            "23          relief     0.045076\n",
            "27         neutral     0.042602\n",
            "14            fear     0.041618\n",
            "9   disappointment     0.041075\n",
            "15       gratitude     0.040307\n",
            "26        surprise     0.039212\n",
            "2            anger     0.038261\n",
            "4         approval     0.036417\n",
            "1        amusement     0.035945\n",
            "19     nervousness     0.034107\n",
            "20        optimism     0.032960\n",
            "24         remorse     0.032921\n",
            "22     realization     0.031503\n",
            "16           grief     0.029583\n",
            "21           pride     0.028891\n",
            "7        curiosity     0.026949\n",
            "6        confusion     0.026020\n",
            "12   embarrassment     0.025936\n",
            "8           desire     0.025678\n",
            "11         disgust     0.020666\n",
            "3        annoyance     0.020622\n",
            "10     disapproval     0.015628\n",
            "==================================================\n",
            "Text: 'This is the best day ever'\n",
            "           Emotion  Probability\n",
            "21           pride     0.053256\n",
            "13      excitement     0.048828\n",
            "17             joy     0.047780\n",
            "0       admiration     0.046635\n",
            "23          relief     0.042830\n",
            "9   disappointment     0.042494\n",
            "26        surprise     0.042364\n",
            "27         neutral     0.042085\n",
            "2            anger     0.041472\n",
            "18            love     0.040791\n",
            "5           caring     0.040415\n",
            "14            fear     0.040021\n",
            "7        curiosity     0.039077\n",
            "20        optimism     0.038407\n",
            "4         approval     0.038003\n",
            "22     realization     0.035845\n",
            "8           desire     0.032424\n",
            "1        amusement     0.032316\n",
            "15       gratitude     0.029515\n",
            "3        annoyance     0.029103\n",
            "6        confusion     0.028759\n",
            "25         sadness     0.028419\n",
            "16           grief     0.026976\n",
            "11         disgust     0.026701\n",
            "12   embarrassment     0.024639\n",
            "24         remorse     0.024491\n",
            "10     disapproval     0.019090\n",
            "19     nervousness     0.017264\n",
            "==================================================\n",
            "Text: 'I'm really excited about the future'\n",
            "           Emotion  Probability\n",
            "13      excitement     0.053657\n",
            "26        surprise     0.052016\n",
            "17             joy     0.049342\n",
            "27         neutral     0.046446\n",
            "9   disappointment     0.045005\n",
            "0       admiration     0.044449\n",
            "19     nervousness     0.044357\n",
            "2            anger     0.042938\n",
            "7        curiosity     0.042342\n",
            "6        confusion     0.041213\n",
            "4         approval     0.039649\n",
            "20        optimism     0.038253\n",
            "1        amusement     0.037732\n",
            "22     realization     0.036409\n",
            "5           caring     0.034642\n",
            "14            fear     0.034137\n",
            "8           desire     0.032492\n",
            "15       gratitude     0.030698\n",
            "12   embarrassment     0.028943\n",
            "25         sadness     0.028658\n",
            "18            love     0.027723\n",
            "24         remorse     0.027714\n",
            "3        annoyance     0.027269\n",
            "23          relief     0.026508\n",
            "21           pride     0.025029\n",
            "16           grief     0.022016\n",
            "11         disgust     0.021513\n",
            "10     disapproval     0.018851\n",
            "==================================================\n",
            "Text: 'I feel so grateful for my friends'\n",
            "           Emotion  Probability\n",
            "23          relief     0.046885\n",
            "21           pride     0.046676\n",
            "24         remorse     0.046612\n",
            "19     nervousness     0.045403\n",
            "14            fear     0.044636\n",
            "12   embarrassment     0.043749\n",
            "13      excitement     0.043734\n",
            "25         sadness     0.043312\n",
            "17             joy     0.043080\n",
            "15       gratitude     0.042199\n",
            "18            love     0.042094\n",
            "5           caring     0.039585\n",
            "8           desire     0.038376\n",
            "9   disappointment     0.036233\n",
            "0       admiration     0.036042\n",
            "27         neutral     0.034481\n",
            "20        optimism     0.032971\n",
            "2            anger     0.032508\n",
            "4         approval     0.032079\n",
            "1        amusement     0.031595\n",
            "22     realization     0.030057\n",
            "26        surprise     0.028940\n",
            "16           grief     0.028261\n",
            "3        annoyance     0.027932\n",
            "11         disgust     0.024140\n",
            "7        curiosity     0.022697\n",
            "6        confusion     0.022391\n",
            "10     disapproval     0.013332\n",
            "==================================================\n",
            "Text: 'I'm excited but also nervous about the presentation'\n",
            "           Emotion  Probability\n",
            "19     nervousness     0.055322\n",
            "14            fear     0.050797\n",
            "13      excitement     0.050490\n",
            "26        surprise     0.048787\n",
            "17             joy     0.047468\n",
            "27         neutral     0.044541\n",
            "9   disappointment     0.043452\n",
            "6        confusion     0.043407\n",
            "0       admiration     0.040746\n",
            "4         approval     0.038371\n",
            "2            anger     0.037882\n",
            "22     realization     0.037251\n",
            "7        curiosity     0.036747\n",
            "25         sadness     0.036450\n",
            "12   embarrassment     0.036065\n",
            "20        optimism     0.035700\n",
            "1        amusement     0.033777\n",
            "5           caring     0.033039\n",
            "15       gratitude     0.029329\n",
            "23          relief     0.028677\n",
            "24         remorse     0.028661\n",
            "8           desire     0.027751\n",
            "3        annoyance     0.025385\n",
            "10     disapproval     0.024513\n",
            "21           pride     0.023402\n",
            "11         disgust     0.020814\n",
            "18            love     0.020637\n",
            "16           grief     0.020538\n",
            "==================================================\n",
            "Text: 'I'm happy to see you, but also feel a bit sad'\n",
            "           Emotion  Probability\n",
            "16           grief     0.045837\n",
            "24         remorse     0.045615\n",
            "25         sadness     0.045355\n",
            "23          relief     0.045218\n",
            "14            fear     0.045093\n",
            "19     nervousness     0.045061\n",
            "8           desire     0.043713\n",
            "12   embarrassment     0.043322\n",
            "17             joy     0.043013\n",
            "13      excitement     0.041649\n",
            "11         disgust     0.040573\n",
            "5           caring     0.038392\n",
            "9   disappointment     0.037886\n",
            "20        optimism     0.036981\n",
            "27         neutral     0.033784\n",
            "26        surprise     0.033645\n",
            "4         approval     0.032732\n",
            "0       admiration     0.031326\n",
            "22     realization     0.031054\n",
            "3        annoyance     0.030876\n",
            "15       gratitude     0.029390\n",
            "1        amusement     0.028541\n",
            "6        confusion     0.026780\n",
            "2            anger     0.026776\n",
            "10     disapproval     0.026698\n",
            "18            love     0.026410\n",
            "7        curiosity     0.024225\n",
            "21           pride     0.020054\n",
            "==================================================\n",
            "Text: 'I feel proud but anxious about the exam results'\n",
            "           Emotion  Probability\n",
            "19     nervousness     0.048919\n",
            "14            fear     0.048851\n",
            "12   embarrassment     0.046802\n",
            "25         sadness     0.044987\n",
            "24         remorse     0.044211\n",
            "17             joy     0.043400\n",
            "13      excitement     0.042649\n",
            "23          relief     0.042623\n",
            "5           caring     0.041706\n",
            "21           pride     0.039014\n",
            "9   disappointment     0.038716\n",
            "27         neutral     0.038370\n",
            "26        surprise     0.036479\n",
            "2            anger     0.035747\n",
            "4         approval     0.033568\n",
            "20        optimism     0.033251\n",
            "0       admiration     0.033058\n",
            "3        annoyance     0.032419\n",
            "22     realization     0.031607\n",
            "8           desire     0.030556\n",
            "10     disapproval     0.028796\n",
            "6        confusion     0.028024\n",
            "7        curiosity     0.027954\n",
            "16           grief     0.027805\n",
            "11         disgust     0.027654\n",
            "15       gratitude     0.027546\n",
            "1        amusement     0.023082\n",
            "18            love     0.022205\n",
            "==================================================\n",
            "Text: 'I'm happy but also a little scared about the changes'\n",
            "           Emotion  Probability\n",
            "14            fear     0.051868\n",
            "19     nervousness     0.049975\n",
            "23          relief     0.047136\n",
            "17             joy     0.045259\n",
            "13      excitement     0.044376\n",
            "26        surprise     0.042182\n",
            "25         sadness     0.041007\n",
            "8           desire     0.040541\n",
            "9   disappointment     0.040358\n",
            "27         neutral     0.040174\n",
            "5           caring     0.039112\n",
            "20        optimism     0.036430\n",
            "4         approval     0.035873\n",
            "2            anger     0.035424\n",
            "12   embarrassment     0.035194\n",
            "22     realization     0.034677\n",
            "0       admiration     0.034636\n",
            "6        confusion     0.033331\n",
            "10     disapproval     0.032782\n",
            "15       gratitude     0.031435\n",
            "3        annoyance     0.031364\n",
            "7        curiosity     0.029001\n",
            "24         remorse     0.028291\n",
            "21           pride     0.027342\n",
            "11         disgust     0.027124\n",
            "18            love     0.022508\n",
            "1        amusement     0.022360\n",
            "16           grief     0.020241\n",
            "==================================================\n",
            "Text: 'I’m feeling joyful and overwhelmed at the same time'\n",
            "           Emotion  Probability\n",
            "19     nervousness     0.052997\n",
            "12   embarrassment     0.047654\n",
            "13      excitement     0.047427\n",
            "25         sadness     0.045195\n",
            "17             joy     0.044469\n",
            "26        surprise     0.044166\n",
            "23          relief     0.043686\n",
            "27         neutral     0.042621\n",
            "9   disappointment     0.042434\n",
            "2            anger     0.039376\n",
            "5           caring     0.038771\n",
            "0       admiration     0.037818\n",
            "4         approval     0.037054\n",
            "22     realization     0.036303\n",
            "3        annoyance     0.035750\n",
            "14            fear     0.035196\n",
            "20        optimism     0.034204\n",
            "11         disgust     0.033063\n",
            "7        curiosity     0.031875\n",
            "15       gratitude     0.029061\n",
            "6        confusion     0.028757\n",
            "18            love     0.027469\n",
            "1        amusement     0.026707\n",
            "24         remorse     0.026153\n",
            "21           pride     0.023616\n",
            "16           grief     0.023538\n",
            "8           desire     0.022642\n",
            "10     disapproval     0.021998\n",
            "==================================================\n",
            "Text: 'I just woke up'\n",
            "           Emotion  Probability\n",
            "16           grief     0.050928\n",
            "19     nervousness     0.049953\n",
            "23          relief     0.049791\n",
            "14            fear     0.048706\n",
            "26        surprise     0.045608\n",
            "13      excitement     0.045322\n",
            "12   embarrassment     0.043422\n",
            "25         sadness     0.042651\n",
            "2            anger     0.041532\n",
            "27         neutral     0.040267\n",
            "9   disappointment     0.039809\n",
            "11         disgust     0.039572\n",
            "3        annoyance     0.038813\n",
            "17             joy     0.035088\n",
            "1        amusement     0.033511\n",
            "4         approval     0.033431\n",
            "20        optimism     0.031874\n",
            "22     realization     0.031585\n",
            "5           caring     0.030795\n",
            "7        curiosity     0.030184\n",
            "0       admiration     0.029602\n",
            "24         remorse     0.028908\n",
            "6        confusion     0.026466\n",
            "15       gratitude     0.025297\n",
            "10     disapproval     0.023595\n",
            "8           desire     0.023398\n",
            "21           pride     0.021423\n",
            "18            love     0.018468\n",
            "==================================================\n",
            "Text: 'The weather is fine today'\n",
            "           Emotion  Probability\n",
            "23          relief     0.056984\n",
            "13      excitement     0.051847\n",
            "27         neutral     0.045838\n",
            "9   disappointment     0.045590\n",
            "0       admiration     0.045565\n",
            "2            anger     0.044839\n",
            "26        surprise     0.044023\n",
            "17             joy     0.043907\n",
            "4         approval     0.040730\n",
            "5           caring     0.038384\n",
            "7        curiosity     0.037966\n",
            "22     realization     0.036451\n",
            "20        optimism     0.036224\n",
            "25         sadness     0.035470\n",
            "3        annoyance     0.035002\n",
            "14            fear     0.034432\n",
            "15       gratitude     0.034144\n",
            "10     disapproval     0.034094\n",
            "6        confusion     0.032823\n",
            "11         disgust     0.030782\n",
            "1        amusement     0.029510\n",
            "12   embarrassment     0.029414\n",
            "24         remorse     0.028012\n",
            "16           grief     0.023068\n",
            "21           pride     0.022465\n",
            "18            love     0.022425\n",
            "8           desire     0.022018\n",
            "19     nervousness     0.017991\n",
            "==================================================\n",
            "Text: 'I need to go to work'\n",
            "           Emotion  Probability\n",
            "8           desire     0.056157\n",
            "13      excitement     0.051784\n",
            "5           caring     0.050611\n",
            "27         neutral     0.046209\n",
            "20        optimism     0.045803\n",
            "19     nervousness     0.045773\n",
            "2            anger     0.044238\n",
            "9   disappointment     0.043843\n",
            "17             joy     0.042361\n",
            "4         approval     0.039661\n",
            "23          relief     0.038919\n",
            "3        annoyance     0.038834\n",
            "7        curiosity     0.038202\n",
            "0       admiration     0.036996\n",
            "6        confusion     0.035177\n",
            "22     realization     0.033880\n",
            "26        surprise     0.033716\n",
            "15       gratitude     0.033381\n",
            "10     disapproval     0.032181\n",
            "24         remorse     0.031803\n",
            "25         sadness     0.031563\n",
            "21           pride     0.024016\n",
            "18            love     0.023908\n",
            "1        amusement     0.023287\n",
            "12   embarrassment     0.022890\n",
            "11         disgust     0.019811\n",
            "14            fear     0.018467\n",
            "16           grief     0.016530\n",
            "==================================================\n",
            "Text: 'The meeting is scheduled for 3 PM'\n",
            "           Emotion  Probability\n",
            "13      excitement     0.056328\n",
            "27         neutral     0.050273\n",
            "9   disappointment     0.048850\n",
            "2            anger     0.047851\n",
            "26        surprise     0.047760\n",
            "17             joy     0.045225\n",
            "0       admiration     0.044959\n",
            "7        curiosity     0.044013\n",
            "4         approval     0.043316\n",
            "22     realization     0.039413\n",
            "6        confusion     0.038040\n",
            "20        optimism     0.037896\n",
            "5           caring     0.036139\n",
            "1        amusement     0.035682\n",
            "25         sadness     0.033009\n",
            "15       gratitude     0.032489\n",
            "3        annoyance     0.032057\n",
            "21           pride     0.031066\n",
            "10     disapproval     0.030872\n",
            "24         remorse     0.029424\n",
            "23          relief     0.028316\n",
            "14            fear     0.026892\n",
            "8           desire     0.026034\n",
            "11         disgust     0.025856\n",
            "18            love     0.025832\n",
            "12   embarrassment     0.025195\n",
            "16           grief     0.020800\n",
            "19     nervousness     0.016414\n",
            "==================================================\n",
            "Text: 'I'm reading a book right now'\n",
            "           Emotion  Probability\n",
            "23          relief     0.053270\n",
            "13      excitement     0.050758\n",
            "26        surprise     0.047571\n",
            "17             joy     0.045900\n",
            "27         neutral     0.044050\n",
            "9   disappointment     0.043566\n",
            "2            anger     0.041758\n",
            "0       admiration     0.039932\n",
            "4         approval     0.039218\n",
            "6        confusion     0.038516\n",
            "7        curiosity     0.036831\n",
            "20        optimism     0.036727\n",
            "22     realization     0.036388\n",
            "1        amusement     0.036182\n",
            "3        annoyance     0.035520\n",
            "8           desire     0.033986\n",
            "19     nervousness     0.033208\n",
            "5           caring     0.032576\n",
            "18            love     0.031229\n",
            "25         sadness     0.030529\n",
            "24         remorse     0.030008\n",
            "12   embarrassment     0.029668\n",
            "15       gratitude     0.029619\n",
            "14            fear     0.029570\n",
            "21           pride     0.026849\n",
            "10     disapproval     0.023092\n",
            "11         disgust     0.023051\n",
            "16           grief     0.020428\n",
            "==================================================\n",
            "Text: 'I don’t know what to feel about this'\n",
            "           Emotion  Probability\n",
            "19     nervousness     0.053731\n",
            "6        confusion     0.048372\n",
            "26        surprise     0.047306\n",
            "13      excitement     0.047105\n",
            "8           desire     0.046703\n",
            "7        curiosity     0.044865\n",
            "27         neutral     0.043224\n",
            "5           caring     0.043111\n",
            "2            anger     0.042830\n",
            "9   disappointment     0.042371\n",
            "3        annoyance     0.039731\n",
            "17             joy     0.038847\n",
            "4         approval     0.037121\n",
            "20        optimism     0.036434\n",
            "25         sadness     0.035839\n",
            "22     realization     0.034901\n",
            "12   embarrassment     0.033790\n",
            "0       admiration     0.033186\n",
            "24         remorse     0.031520\n",
            "10     disapproval     0.029123\n",
            "15       gratitude     0.028322\n",
            "11         disgust     0.027814\n",
            "1        amusement     0.026876\n",
            "14            fear     0.024361\n",
            "23          relief     0.023474\n",
            "18            love     0.022328\n",
            "16           grief     0.022260\n",
            "21           pride     0.014453\n",
            "==================================================\n",
            "Text: 'It’s hard to say how I feel'\n",
            "           Emotion  Probability\n",
            "19     nervousness     0.049800\n",
            "12   embarrassment     0.047595\n",
            "25         sadness     0.046171\n",
            "26        surprise     0.044776\n",
            "8           desire     0.044104\n",
            "13      excitement     0.043691\n",
            "6        confusion     0.043234\n",
            "5           caring     0.042099\n",
            "2            anger     0.041084\n",
            "27         neutral     0.040948\n",
            "9   disappointment     0.040829\n",
            "3        annoyance     0.039992\n",
            "17             joy     0.038766\n",
            "24         remorse     0.038143\n",
            "7        curiosity     0.037877\n",
            "4         approval     0.035288\n",
            "0       admiration     0.034380\n",
            "20        optimism     0.034180\n",
            "22     realization     0.033099\n",
            "10     disapproval     0.031412\n",
            "15       gratitude     0.028444\n",
            "16           grief     0.027920\n",
            "11         disgust     0.027719\n",
            "1        amusement     0.025606\n",
            "23          relief     0.022904\n",
            "18            love     0.021836\n",
            "14            fear     0.021598\n",
            "21           pride     0.016506\n",
            "==================================================\n",
            "Text: 'I have mixed feelings about this'\n",
            "           Emotion  Probability\n",
            "13      excitement     0.048881\n",
            "6        confusion     0.047992\n",
            "19     nervousness     0.046744\n",
            "26        surprise     0.045603\n",
            "27         neutral     0.045574\n",
            "9   disappointment     0.044733\n",
            "2            anger     0.043232\n",
            "17             joy     0.041530\n",
            "5           caring     0.040233\n",
            "7        curiosity     0.038857\n",
            "4         approval     0.038650\n",
            "25         sadness     0.037883\n",
            "0       admiration     0.036888\n",
            "10     disapproval     0.036877\n",
            "22     realization     0.036174\n",
            "3        annoyance     0.035061\n",
            "12   embarrassment     0.034845\n",
            "20        optimism     0.034830\n",
            "24         remorse     0.033663\n",
            "11         disgust     0.029633\n",
            "15       gratitude     0.029559\n",
            "1        amusement     0.028719\n",
            "23          relief     0.026331\n",
            "18            love     0.025914\n",
            "8           desire     0.025866\n",
            "14            fear     0.025401\n",
            "16           grief     0.022043\n",
            "21           pride     0.018285\n",
            "==================================================\n",
            "Text: 'I’m trying to understand my emotions'\n",
            "           Emotion  Probability\n",
            "24         remorse     0.048409\n",
            "19     nervousness     0.048348\n",
            "12   embarrassment     0.045650\n",
            "21           pride     0.045070\n",
            "23          relief     0.045035\n",
            "13      excitement     0.044019\n",
            "25         sadness     0.041745\n",
            "8           desire     0.040875\n",
            "17             joy     0.038503\n",
            "2            anger     0.038359\n",
            "9   disappointment     0.037677\n",
            "27         neutral     0.037578\n",
            "14            fear     0.036990\n",
            "3        annoyance     0.034610\n",
            "26        surprise     0.034136\n",
            "5           caring     0.033830\n",
            "18            love     0.032784\n",
            "4         approval     0.032780\n",
            "0       admiration     0.031532\n",
            "20        optimism     0.031428\n",
            "1        amusement     0.031238\n",
            "22     realization     0.030401\n",
            "6        confusion     0.028921\n",
            "11         disgust     0.028555\n",
            "7        curiosity     0.027259\n",
            "15       gratitude     0.026742\n",
            "10     disapproval     0.025142\n",
            "16           grief     0.022385\n",
            "==================================================\n",
            "Text: 'It’s complicated'\n",
            "           Emotion  Probability\n",
            "13      excitement     0.055643\n",
            "27         neutral     0.050535\n",
            "26        surprise     0.049017\n",
            "2            anger     0.048895\n",
            "9   disappointment     0.048232\n",
            "17             joy     0.044080\n",
            "0       admiration     0.043351\n",
            "4         approval     0.042426\n",
            "6        confusion     0.040903\n",
            "1        amusement     0.038967\n",
            "7        curiosity     0.037537\n",
            "20        optimism     0.036786\n",
            "22     realization     0.036689\n",
            "12   embarrassment     0.035163\n",
            "25         sadness     0.033751\n",
            "5           caring     0.033699\n",
            "15       gratitude     0.032665\n",
            "14            fear     0.031714\n",
            "24         remorse     0.031118\n",
            "11         disgust     0.030698\n",
            "3        annoyance     0.027902\n",
            "18            love     0.027361\n",
            "23          relief     0.026464\n",
            "19     nervousness     0.025750\n",
            "10     disapproval     0.025188\n",
            "16           grief     0.024118\n",
            "8           desire     0.021487\n",
            "21           pride     0.019858\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#white llm"
      ],
      "metadata": {
        "id": "_p4ZpxsWKo2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Tokenizer and model for BERT\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load BERT model with 28 output labels (for 28 emotions)\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=28).to(device)\n",
        "\n",
        "# Function to predict using BERT for multiple emotions and apply Softmax\n",
        "def predict_bert(text, tokenizer, bert_model, device):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=-1).squeeze().cpu().numpy()  # Apply Softmax to logits\n",
        "    return probs\n",
        "\n",
        "# Example text for prediction\n",
        "text = \"I feel so sad\"\n",
        "probs_bert = predict_bert(text, tokenizer, bert_model, device)\n",
        "\n",
        "# Emotion labels for the 28 emotions\n",
        "emotion_labels = emotion_columns\n",
        "\n",
        "# Create a DataFrame with the emotions and BERT's probabilities\n",
        "df_bert_predictions = pd.DataFrame({\n",
        "    'Emotion': emotion_labels,\n",
        "    'BERT Probability': probs_bert\n",
        "})\n",
        "\n",
        "# Get the emotion with the highest probability\n",
        "df_bert_predictions['Max Probability'] = df_bert_predictions['BERT Probability'].max()\n",
        "df_bert_predictions['BERT Prediction'] = np.where(df_bert_predictions['BERT Probability'] == df_bert_predictions['Max Probability'], 1, 0)\n",
        "\n",
        "# Sort by the highest probability\n",
        "df_bert_predictions = df_bert_predictions.sort_values(by='BERT Probability', ascending=False)\n",
        "\n",
        "# Show the final predictions for BERT\n",
        "print(df_bert_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrUu48iqyAWj",
        "outputId": "9d76e242-0861-4cc0-d100-17138f88d5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Emotion  BERT Probability  Max Probability  BERT Prediction\n",
            "23          relief          0.051065         0.051065                1\n",
            "14            fear          0.049554         0.051065                0\n",
            "7        curiosity          0.049232         0.051065                0\n",
            "15       gratitude          0.047803         0.051065                0\n",
            "1        amusement          0.044621         0.051065                0\n",
            "11         disgust          0.043542         0.051065                0\n",
            "2            anger          0.043074         0.051065                0\n",
            "6        confusion          0.042843         0.051065                0\n",
            "27         neutral          0.041821         0.051065                0\n",
            "18            love          0.040525         0.051065                0\n",
            "10     disapproval          0.038225         0.051065                0\n",
            "20        optimism          0.037677         0.051065                0\n",
            "16           grief          0.035970         0.051065                0\n",
            "24         remorse          0.034592         0.051065                0\n",
            "8           desire          0.031912         0.051065                0\n",
            "25         sadness          0.031561         0.051065                0\n",
            "3        annoyance          0.031299         0.051065                0\n",
            "17             joy          0.031220         0.051065                0\n",
            "12   embarrassment          0.031016         0.051065                0\n",
            "22     realization          0.030841         0.051065                0\n",
            "0       admiration          0.030133         0.051065                0\n",
            "21           pride          0.029606         0.051065                0\n",
            "13      excitement          0.027463         0.051065                0\n",
            "19     nervousness          0.027416         0.051065                0\n",
            "5           caring          0.027001         0.051065                0\n",
            "26        surprise          0.026358         0.051065                0\n",
            "9   disappointment          0.023451         0.051065                0\n",
            "4         approval          0.020178         0.051065                0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Tokenizer and model for BERT\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Function to load trained models\n",
        "def load_trained_models(emotion_models, model_class, device):\n",
        "    trained_models = {}\n",
        "    for emotion in emotion_models:  # Iterate over the list of emotions\n",
        "        model_filename = f'best_model_{emotion}.pt'\n",
        "        if os.path.exists(model_filename):\n",
        "            model = model_class()\n",
        "            model.load_state_dict(torch.load(model_filename, map_location=device))\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "            trained_models[emotion] = model\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: Model file {model_filename} not found, skipping {emotion}\")\n",
        "\n",
        "    return trained_models\n",
        "\n",
        "# Function to predict using the ensemble of models\n",
        "def predict_ensemble(text, trained_models, word_vectors, device):\n",
        "    text_vectorized = [word_vectors.get(word, np.zeros(300)) for word in text.split()]\n",
        "    text_vectorized = text_vectorized[:100] if len(text_vectorized) > 100 else text_vectorized + [np.zeros(300)] * (100 - len(text_vectorized))\n",
        "    text_tensor = torch.tensor(np.array(text_vectorized), dtype=torch.float).unsqueeze(0).to(device)\n",
        "\n",
        "    emotion_probs = {}\n",
        "    for emotion, model in trained_models.items():\n",
        "        with torch.no_grad():\n",
        "            output = model(text_tensor)\n",
        "            prob = torch.sigmoid(output).item()\n",
        "            emotion_probs[emotion] = prob\n",
        "\n",
        "    total_prob = sum(emotion_probs.values())\n",
        "    for emotion in emotion_probs:\n",
        "        emotion_probs[emotion] /= total_prob if total_prob > 0 else 1\n",
        "\n",
        "    df = pd.DataFrame(list(emotion_probs.items()), columns=['Emotion', 'Probability'])\n",
        "    df = df.sort_values(by='Probability', ascending=False)\n",
        "    return df\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def calculate_cosine_similarity(probs_1, probs_2):\n",
        "    return cosine_similarity([probs_1], [probs_2])[0][0]\n",
        "\n",
        "# Function to calculate Euclidean distance\n",
        "def calculate_euclidean_distance(probs_1, probs_2):\n",
        "    return np.linalg.norm(np.array(probs_1) - np.array(probs_2))\n",
        "\n",
        "# Load BERT model\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name).to(device)\n",
        "\n",
        "# Function to predict using BERT\n",
        "def predict_bert(text, tokenizer, bert_model, device):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "# Example text for prediction\n",
        "text = \"I feel so sad\"\n",
        "\n",
        "# Calculate probabilities for your ensemble model\n",
        "trained_models = load_trained_models(emotion_models, EmotionLSTMWithAttention, device)\n",
        "result_df = predict_ensemble(text, trained_models, word_vectors, device)\n",
        "emotion_probs_ensemble = result_df['Probability'].values  # Probabilities from the ensemble\n",
        "\n",
        "# Calculate probabilities for BERT\n",
        "probs_bert = predict_bert(text, tokenizer, bert_model, device)\n",
        "\n",
        "\n",
        "bert_probs_adjusted = np.zeros(28)  # Create a 28-length array\n",
        "bert_probs_adjusted[:2] = probs_bert  # Assign the first 2 probabilities from BERT\n",
        "\n",
        "# Now calculate cosine similarity and Euclidean distance using the same dimensions\n",
        "cosine_sim = calculate_cosine_similarity(emotion_probs_ensemble, bert_probs_adjusted)\n",
        "euclidean_dist = calculate_euclidean_distance(emotion_probs_ensemble, bert_probs_adjusted)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Cosine Similarity: {cosine_sim}\")\n",
        "print(f\"Euclidean Distance: {euclidean_dist}\")\n",
        "\n",
        "# Combine the probabilities from the ensemble model and BERT\n",
        "combined_probs = np.vstack([emotion_probs_ensemble, bert_probs_adjusted]).T  # Stack them side by side\n",
        "\n",
        "# Create the DataFrame with appropriate column names\n",
        "df_comparison = pd.DataFrame(combined_probs, columns=['Ensemble Probability', 'BERT Probability'])\n",
        "\n",
        "# Optionally, if you want to see which emotions BERT is matching\n",
        "df_comparison['Emotion'] = result_df['Emotion'].values\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_comparison)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy1yH0F0v7lM",
        "outputId": "e1b5539b-bf8a-45a8-98a0-549d74020612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.3459254372478616\n",
            "Euclidean Distance: 0.7708719591225267\n",
            "           Emotion  Ensemble Prediction  BERT Prediction\n",
            "0            grief                    0                1\n",
            "1          remorse                    0                1\n",
            "2          sadness                    0                0\n",
            "3             fear                    0                0\n",
            "4           relief                    0                0\n",
            "5      nervousness                    0                0\n",
            "6    embarrassment                    0                0\n",
            "7              joy                    0                0\n",
            "8          disgust                    0                0\n",
            "9       excitement                    0                0\n",
            "10  disappointment                    0                0\n",
            "11          caring                    0                0\n",
            "12        surprise                    0                0\n",
            "13         neutral                    0                0\n",
            "14           anger                    0                0\n",
            "15       annoyance                    0                0\n",
            "16      admiration                    0                0\n",
            "17       amusement                    0                0\n",
            "18        approval                    0                0\n",
            "19        optimism                    0                0\n",
            "20     realization                    0                0\n",
            "21            love                    0                0\n",
            "22          desire                    0                0\n",
            "23       gratitude                    0                0\n",
            "24       curiosity                    0                0\n",
            "25     disapproval                    0                0\n",
            "26       confusion                    0                0\n",
            "27           pride                    0                0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EmotionTextDataset(Dataset):\n",
        "    def __init__(self, texts, emotion_probs, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.emotion_probs = emotion_probs\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        emotion_prob = torch.tensor(self.emotion_probs[idx], dtype=torch.float)\n",
        "        # Tokenize the text\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "        # We return the input ids and attention mask together with the emotion probabilities\n",
        "        return inputs['input_ids'].squeeze(0), emotion_prob\n",
        "\n",
        "\n",
        "\n",
        "class EmotionAugmentedLLM(nn.Module):\n",
        "    def __init__(self, llm_model_name, emotion_models):\n",
        "        super(EmotionAugmentedLLM, self).__init__()\n",
        "        # Load the large language model (LLM) for sequence classification\n",
        "        self.llm = AutoModelForSequenceClassification.from_pretrained(llm_model_name)\n",
        "        # Store the emotion models\n",
        "        self.emotion_models = emotion_models\n",
        "        # Add a layer to combine the LLM output with emotion probabilities\n",
        "        self.emotion_fc = nn.Linear(self.llm.config.hidden_size + len(emotion_models), 2)  # Example binary classification\n",
        "\n",
        "    def forward(self, text, emotion_features):\n",
        "        # Pass text through the LLM model\n",
        "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "        llm_output = self.llm(**inputs)\n",
        "        # Concatenate LLM output with emotion features\n",
        "        combined_features = torch.cat((llm_output.logits, emotion_features), dim=-1)\n",
        "        # Pass combined features through a fully connected layer\n",
        "        output = self.emotion_fc(combined_features)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def get_emotion_probabilities(text, emotion_models):\n",
        "    emotion_probs = []\n",
        "    for model in emotion_models:\n",
        "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "        emotion_prob = model(inputs['input_ids'], inputs['attention_mask']).squeeze().cpu().detach().numpy()\n",
        "        emotion_probs.append(emotion_prob)\n",
        "    return torch.tensor(emotion_probs).unsqueeze(0).to(device)  # [1, num_emotions]\n",
        "\n",
        "# Instantiate models\n",
        "llm_model_name = \"bert-base-uncased\"  # Example LLM (you can choose a different one)\n",
        "emotion_models = [EmotionLSTMWithAttention() for _ in range(5)]  # Replace with real emotion models\n",
        "\n",
        "# Create the Emotion-AUG LLM model\n",
        "emotion_augmented_llm = EmotionAugmentedLLM(llm_model_name, emotion_models).to(device)\n",
        "\n",
        "# Example text and emotion probabilities (for demo)\n",
        "texts = [\"I am so happy today!\", \"I feel so sad...\"]\n",
        "emotion_probs = [\n",
        "    [0.9, 0.05, 0.1, 0.2, 0.05],  # Example emotion probabilities for the first text\n",
        "    [0.1, 0.9, 0.2, 0.5, 0.05]   # Example emotion probabilities for the second text\n",
        "]\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = EmotionTextDataset(texts, emotion_probs, tokenizer=AutoTokenizer.from_pretrained(llm_model_name))\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "optimizer = optim.Adam(emotion_augmented_llm.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    emotion_augmented_llm.train()\n",
        "    running_loss = 0.0\n",
        "    for text, emotion_labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get emotion probabilities\n",
        "        emotion_features = get_emotion_probabilities(text, emotion_models)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = emotion_augmented_llm(text, emotion_features)\n",
        "\n",
        "        # Calculate loss and backpropagate\n",
        "        loss = criterion(outputs, emotion_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {running_loss/len(dataloader)}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(emotion_augmented_llm.state_dict(), \"emotion_augmented_llm.pth\")"
      ],
      "metadata": {
        "id": "f5xEUYCyKqNx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}